{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        </script>\n",
       "        <script type=\"module\">import \"https://cdn.plot.ly/plotly-3.0.1.min\"</script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "survived\n",
       "0.0    808\n",
       "1.0    500\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "pd.set_option(\"display.max_columns\",30)\n",
    "pd.set_option(\"display.max_rows\",3000)\n",
    "import matplotlib.pyplot as plt \n",
    "import cufflinks as cf \n",
    "cf.go_offline()\n",
    "import numpy as np\n",
    "\n",
    "### imputacion variables continuas\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "df = pd.read_csv(\"titanic.csv\")\n",
    "\n",
    "df.shape\n",
    "\n",
    "df.survived.value_counts(True)\n",
    "\n",
    "df\n",
    "\n",
    "df.survived.value_counts(True)\n",
    "\n",
    "def completitud_datos_nulos(df):\n",
    "    return df.isnull().sum().sort_values(ascending=False) / df.shape[0]\n",
    "\n",
    "completitud_datos_nulos(df)\n",
    "\n",
    "df = df[(~df[\"survived\"].isnull()) & ~df[\"pclass\"].isnull() & ~df[\"fare\"].isnull() ]\n",
    "\n",
    "df\n",
    "\n",
    "def complete_continuous_variables(df,col,strategy='median'):\n",
    "    X = df[col].copy()\n",
    "    im = SimpleImputer(strategy=strategy)\n",
    "    Xi = pd.DataFrame(im.fit_transform(X),columns=col)\n",
    "    l_ks = []\n",
    "    for v in col:\n",
    "        l_ks.append([v,ks_2samp(X[v].dropna(),Xi[v]).statistic])\n",
    "    ks = pd.DataFrame(l_ks,columns=['feat','ks'])\n",
    "    #print(ks)\n",
    "    print((ks.ks>=0.1).sum())\n",
    "    df[col] = im.transform(df[col].copy())\n",
    "    return df\n",
    "\n",
    "complete_continuous_variables(df,[\"body\"])\n",
    "\n",
    "completitud_datos_nulos(df)\n",
    "\n",
    "varc = list(df.describe())\n",
    "\n",
    "varc = [x for x in varc if x not in 'survived']\n",
    "\n",
    "vard = [x for x in df.columns if x not in varc+['survived']]\n",
    "\n",
    "tgt = 'survived'\n",
    "\n",
    "df[tgt].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pclass       0\n",
      "survived     0\n",
      "name         0\n",
      "sex          0\n",
      "age          0\n",
      "sibsp        0\n",
      "parch        0\n",
      "ticket       0\n",
      "fare         0\n",
      "cabin        0\n",
      "embarked     0\n",
      "boat         0\n",
      "body         0\n",
      "home.dest    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Identificar columnas numéricas y categóricas\n",
    "num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "cat_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Imputar valores nulos en las columnas numéricas (mediana)\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "df[num_cols] = num_imputer.fit_transform(df[num_cols])\n",
    "\n",
    "# Imputar valores nulos en las columnas categóricas (moda)\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])\n",
    "\n",
    "# Verificar que no haya valores nulos después de la imputación\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELO CLASIFICACION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[varc].copy() #TAD \"Tabla Analitica de Datos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[tgt].copy() #variable objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = MinMaxScaler()\n",
    "Xs = pd.DataFrame(sc.fit_transform(X), columns=varc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt, Xv, yt, yv = train_test_split(Xs,y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar(param, modelo, X,y):\n",
    "    grid = RandomizedSearchCV(param_distributions=param,\n",
    "                             n_jobs=-1,\n",
    "                             n_iter=10,\n",
    "                             cv=4,\n",
    "                             estimator=modelo,\n",
    "                             error_score='raise')\n",
    "    grid.fit(X,y)\n",
    "    return grid, grid.best_estimator_, grid.best_score_, grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metricas(Xt, Xv, yt, yv, modelo):\n",
    "    d = {'train':round(roc_auc_score(y_true=yt, y_score=modelo.predict_proba(Xt)[:,1]),3),\n",
    "         'validate':round(roc_auc_score(y_true=yv, y_score=modelo.predict_proba(Xv)[:,1]),3)\n",
    "        }\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REDES NEURONALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xt = scaler.fit_transform(Xt)\n",
    "Xv = scaler.transform(Xv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_mlpc = dict(\n",
    "    hidden_layer_sizes = [(50,), (100,), (150,), (50, 50), (100, 100), (150, 150), (100, 50)],\n",
    "    activation = ['relu', 'tanh'],\n",
    "    solver = ['adam'],\n",
    "    alpha = np.logspace(-5, 0, 20),\n",
    "    learning_rate = ['adaptive'],\n",
    "    learning_rate_init = np.logspace(-4, -1, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': np.float64(0.778), 'validate': np.float64(0.785)}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_mlpc = MLPClassifier(max_iter=2000)  # Mayor número de iteraciones para asegurar convergencia\n",
    "\n",
    "# RandomizedSearchCV para un tuning más eficiente\n",
    "search = RandomizedSearchCV(modelo_mlpc, param_mlpc, n_iter=50, scoring='accuracy', cv=5, random_state=0, n_jobs=-1)\n",
    "search.fit(Xt, yt)\n",
    "\n",
    "best_estimator_mlpc = search.best_estimator_\n",
    "score_mlpc = search.best_score_\n",
    "params_mlpc = search.best_params_\n",
    "\n",
    "# Evaluación del modelo en los datos de validación\n",
    "metricas(Xt, Xv, yt, yv, best_estimator_mlpc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xt = scaler.fit_transform(Xt)\n",
    "Xv = scaler.transform(Xv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_mlpc = dict(hidden_layer_sizes = [(a,b,c,) for a in range(len(varc), len(varc)*2) for b in range(len(varc), len(varc)*2) for c in range(len(varc), len(varc)*2)],\n",
    "             activation = ['identity', 'logistic', 'tanh', 'relu'],\n",
    "             solver = ['lbfgs', 'sgd', 'adam'],\n",
    "             alpha = np.arange(0.0001, 0.001, 0.0001),\n",
    "             learning_rate = ['constant', 'invscaling', 'adaptive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': np.float64(0.785), 'validate': np.float64(0.789)}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_mlpc = MLPClassifier()\n",
    "modelo_mlpc, best_estimator_mlpc, score_mlpc, params_mlpc = entrenar(param_mlpc, modelo_mlpc, Xt, yt)\n",
    "metricas(Xt,Xv,yt,yv,modelo_mlpc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_mlpc = dict(hidden_layer_sizes = [(a,b,c,) for a in range(len(varc), len(varc)*2) for b in range(len(varc), len(varc)*2) for c in range(len(varc), len(varc)*2)],\n",
    "             activation = ['identity', 'logistic', 'tanh', 'relu'],\n",
    "             solver = ['lbfgs', 'sgd', 'adam'],\n",
    "             alpha = np.arange(0.0001, 0.001, 0.0001),\n",
    "             learning_rate = ['constant', 'invscaling', 'adaptive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': np.float64(0.837), 'validate': np.float64(0.726)}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_mlpc = MLPClassifier(\n",
    "    max_iter=6000,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1\n",
    ")\n",
    "\n",
    "modelo_mlpc, best_estimator_mlpc, score_mlpc, params_mlpc = entrenar(param_mlpc, modelo_mlpc, Xt, yt)\n",
    "metricas(Xt,Xv,yt,yv,modelo_mlpc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trabajar despues del sobre ajuste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xt = scaler.fit_transform(Xt)\n",
    "Xv = scaler.transform(Xv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_mlpc = dict(hidden_layer_sizes = [(a,) for a in range(len(varc), len(varc)+5)],\n",
    "             activation = ['identity', 'logistic', 'tanh', 'relu'],\n",
    "             solver = ['lbfgs', 'sgd', 'adam'],\n",
    "             alpha = np.logspace(-4, 0, 10),\n",
    "             learning_rate = ['constant', 'invscaling', 'adaptive'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': np.float64(0.807), 'validate': np.float64(0.802)}"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_mlpc = MLPClassifier(\n",
    "    max_iter = 5000,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1\n",
    ")\n",
    "\n",
    "modelo_mlpc, best_estimator_mlpc, score_mlpc, params_mlpc = entrenar(param_mlpc, modelo_mlpc, Xt, yt)\n",
    "metricas(Xt,Xv,yt,yv,modelo_mlpc)         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en ./modelo_red_neuronal.pkl\n"
     ]
    }
   ],
   "source": [
    "filename = './modelo_red_neuronal.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(modelo_mlpc,file)\n",
    "    \n",
    "print(f'Modelo guardado en {filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NUEVO MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xt = scaler.fit_transform(Xt)\n",
    "Xv = scaler.transform(Xv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_mlpc = dict(hidden_layer_sizes = [(a,) for a in range(len(varc), len(varc)+5)],\n",
    "             activation = ['identity', 'logistic', 'tanh', 'relu'],\n",
    "             solver = ['lbfgs', 'sgd', 'adam'],\n",
    "             alpha = np.logspace(-4, 0, 10),\n",
    "             learning_rate = ['constant', 'invscaling', 'adaptive'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': np.float64(0.8), 'validate': np.float64(0.827)}"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_mlpc = MLPClassifier(\n",
    "    max_iter = 5000,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1\n",
    ")\n",
    "\n",
    "modelo_mlpc, best_estimator_mlpc, score_mlpc, params_mlpc = entrenar(param_mlpc, modelo_mlpc, Xt, yt)\n",
    "metricas(Xt,Xv,yt,yv,modelo_mlpc)         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en ./modelo_red_neuronal2.pkl\n"
     ]
    }
   ],
   "source": [
    "filename = './modelo_red_neuronal2.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(modelo_mlpc,file)\n",
    "    \n",
    "print(f'Modelo guardado en {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Escalar los datos\n",
    "scaler = StandardScaler()\n",
    "Xt_scaled = scaler.fit_transform(Xt)\n",
    "Xv_scaled = scaler.transform(Xv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_mlpc = dict(hidden_layer_sizes = [(a,b,c,) for a in range(len(varc), len(varc)*2) for b in range(len(varc), len(varc)*2) for c in range(len(varc), len(varc)*2)],\n",
    "             activation = ['identity', 'logistic', 'tanh', 'relu'],\n",
    "             solver = ['lbfgs', 'sgd', 'adam'],\n",
    "             alpha = np.arange(0.00001, 0.001, 0.00001),\n",
    "             learning_rate = ['constant', 'invscaling', 'adaptive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': np.float64(0.826), 'validate': np.float64(0.808)}"
      ]
     },
     "execution_count": 677,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_mlpc = MLPClassifier(\n",
    "    max_iter = 10000,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1\n",
    ")\n",
    "\n",
    "modelo_mlpc, best_estimator_mlpc, score_mlpc, params_mlpc = entrenar(param_mlpc, modelo_mlpc, Xt, yt)\n",
    "metricas(Xt,Xv,yt,yv,modelo_mlpc)         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en ./modelo_red_neuronalfinal.pkl\n"
     ]
    }
   ],
   "source": [
    "filename = './modelo_red_neuronalfinal.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(modelo_mlpc,file)\n",
    "    \n",
    "print(f'Modelo guardado en {filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NUEVO MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes de SMOTE: survived\n",
      "0    651\n",
      "1    395\n",
      "Name: count, dtype: int64\n",
      "Después de SMOTE: survived\n",
      "1    651\n",
      "0    651\n",
      "Name: count, dtype: int64\n",
      "Métricas finales: {'train': np.float64(0.961), 'validate': np.float64(0.819)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, VotingClassifier, RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Balancear las clases con SMOTE\n",
    "print(\"Antes de SMOTE:\", yt.value_counts())\n",
    "smote = SMOTE(random_state=42)\n",
    "Xt_res, yt_res = smote.fit_resample(Xt, yt)\n",
    "print(\"Después de SMOTE:\", pd.Series(yt_res).value_counts())\n",
    "\n",
    "# Crear modelos individuales\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "mlp = MLPClassifier(max_iter=5000, random_state=42)\n",
    "\n",
    "# Crear modelo ensemblado (Voting Classifier)\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', rf),\n",
    "        ('gb', gb),\n",
    "        ('mlp', mlp)\n",
    "    ],\n",
    "    voting='soft'  # Usa probabilidades para combinar las predicciones\n",
    ")\n",
    "\n",
    "# Entrenar el modelo ensemblado\n",
    "ensemble.fit(Xt_res, yt_res)\n",
    "\n",
    "# Evaluar las métricas en los conjuntos de entrenamiento y validación\n",
    "metrics = {\n",
    "    'train': round(roc_auc_score(yt_res, ensemble.predict_proba(Xt_res)[:, 1]), 3),\n",
    "    'validate': round(roc_auc_score(yv, ensemble.predict_proba(Xv)[:, 1]), 3)\n",
    "}\n",
    "\n",
    "print(\"Métricas finales:\", metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes de SMOTE: survived\n",
      "0    651\n",
      "1    395\n",
      "Name: count, dtype: int64\n",
      "Después de SMOTE: survived\n",
      "1    651\n",
      "0    651\n",
      "Name: count, dtype: int64\n",
      "Métricas en AUC:\n",
      "Entrenamiento (train): 0.961\n",
      "Validación (validate): 0.821\n",
      "\n",
      "Métricas en F1-Score:\n",
      "Entrenamiento (train): 0.888\n",
      "Validación (validate): 0.682\n",
      "\n",
      "⚠️ Alerta: Puede haber indicios de sobreajuste. Ajusta los hiperparámetros.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, VotingClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Aplicar SMOTE para balancear clases\n",
    "print(\"Antes de SMOTE:\", yt.value_counts())\n",
    "smote = SMOTE(random_state=42)\n",
    "Xt_res, yt_res = smote.fit_resample(Xt, yt)\n",
    "print(\"Después de SMOTE:\", pd.Series(yt_res).value_counts())\n",
    "\n",
    "# Modelos individuales para el ensamble\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "mlp = MLPClassifier(max_iter=5000, random_state=42, early_stopping=True, validation_fraction=0.1)\n",
    "\n",
    "# Ensamble con VotingClassifier\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', rf),\n",
    "        ('gb', gb),\n",
    "        ('mlp', mlp)\n",
    "    ],\n",
    "    voting='soft'  # Combinación basada en probabilidades\n",
    ")\n",
    "\n",
    "# Entrenar el modelo ensamble con los datos balanceados\n",
    "ensemble.fit(Xt_res, yt_res)\n",
    "\n",
    "# Evaluar el desempeño en conjunto de entrenamiento y validación\n",
    "train_auc = roc_auc_score(yt_res, ensemble.predict_proba(Xt_res)[:, 1])\n",
    "validate_auc = roc_auc_score(yv, ensemble.predict_proba(Xv)[:, 1])\n",
    "\n",
    "train_f1 = f1_score(yt_res, ensemble.predict(Xt_res))\n",
    "validate_f1 = f1_score(yv, ensemble.predict(Xv))\n",
    "\n",
    "# Imprimir métricas\n",
    "print(\"Métricas en AUC:\")\n",
    "print(\"Entrenamiento (train):\", round(train_auc, 3))\n",
    "print(\"Validación (validate):\", round(validate_auc, 3))\n",
    "\n",
    "print(\"\\nMétricas en F1-Score:\")\n",
    "print(\"Entrenamiento (train):\", round(train_f1, 3))\n",
    "print(\"Validación (validate):\", round(validate_f1, 3))\n",
    "\n",
    "# Comprobación de sobreajuste\n",
    "if (train_auc - validate_auc) > 0.05:\n",
    "    print(\"\\n⚠️ Alerta: Puede haber indicios de sobreajuste. Ajusta los hiperparámetros.\")\n",
    "else:\n",
    "    print(\"\\n✅ No se detectó sobreajuste significativo.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento (train): 0.83397\n",
      "Validación (validate): 0.81729\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(\n",
    "    max_iter=5000,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    alpha=0.01\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.05, random_state=42)\n",
    "\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', rf),\n",
    "        ('gb', gb),\n",
    "        ('mlp', mlp)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Entrenar y evaluar\n",
    "ensemble.fit(Xt_res, yt_res)\n",
    "\n",
    "train_auc = roc_auc_score(yt_res, ensemble.predict_proba(Xt_res)[:, 1])\n",
    "validate_auc = roc_auc_score(yv, ensemble.predict_proba(Xv)[:, 1])\n",
    "\n",
    "print(\"Entrenamiento (train):\", round(train_auc, 5))\n",
    "print(\"Validación (validate):\", round(validate_auc, 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MEJORANDO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento (train): 0.849\n",
      "Validación (validate): 0.846\n",
      "Promedio de AUC en validación cruzada: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Importar librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Configuración inicial\n",
    "pd.set_option(\"display.max_columns\", 30)\n",
    "pd.set_option(\"display.max_rows\", 3000)\n",
    "\n",
    "# Cargar datos\n",
    "df = pd.read_csv(\"titanic.csv\")\n",
    "\n",
    "# Función para verificar datos nulos\n",
    "def completitud_datos_nulos(df):\n",
    "    return df.isnull().sum().sort_values(ascending=False) / df.shape[0]\n",
    "\n",
    "# Filtrar datos nulos en columnas clave\n",
    "df = df[(~df[\"survived\"].isnull()) & ~df[\"pclass\"].isnull() & ~df[\"fare\"].isnull()]\n",
    "\n",
    "# Imputar valores numéricos y categóricos\n",
    "num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "cat_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "df[num_cols] = num_imputer.fit_transform(df[num_cols])\n",
    "\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])\n",
    "\n",
    "# Variables objetivo y predictoras\n",
    "tgt = 'survived'\n",
    "varc = list(df.describe())\n",
    "varc = [x for x in varc if x != tgt]\n",
    "vard = [x for x in df.columns if x not in varc + [tgt]]\n",
    "\n",
    "X = df[varc].copy()\n",
    "y = df[tgt].astype(int).copy()\n",
    "\n",
    "# Escalado de datos\n",
    "sc = MinMaxScaler()\n",
    "Xs = pd.DataFrame(sc.fit_transform(X), columns=varc)\n",
    "\n",
    "# División de datos\n",
    "Xt, Xv, yt, yv = train_test_split(Xs, y, train_size=0.8, random_state=42)\n",
    "\n",
    "# Balanceo de datos con SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "Xt_res, yt_res = smote.fit_resample(Xt, yt)\n",
    "\n",
    "# Modelos ajustados\n",
    "mlp = MLPClassifier(\n",
    "    max_iter=5000,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    alpha=0.01,\n",
    "    hidden_layer_sizes=(50, 30, 10)\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=150, max_depth=6, random_state=42)\n",
    "\n",
    "gb = GradientBoostingClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Ensamble ajustado\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', rf),\n",
    "        ('gb', gb),\n",
    "        ('mlp', mlp)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Entrenar el ensamble\n",
    "ensemble.fit(Xt_res, yt_res)\n",
    "\n",
    "# Métricas de AUC\n",
    "train_auc = roc_auc_score(yt_res, ensemble.predict_proba(Xt_res)[:, 1])\n",
    "validate_auc = roc_auc_score(yv, ensemble.predict_proba(Xv)[:, 1])\n",
    "\n",
    "print(\"Entrenamiento (train):\", round(train_auc, 3))\n",
    "print(\"Validación (validate):\", round(validate_auc, 3))\n",
    "\n",
    "# Validación cruzada\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cross_val_scores = cross_val_score(ensemble, Xt_res, yt_res, scoring='roc_auc', cv=cv)\n",
    "print(\"Promedio de AUC en validación cruzada:\", round(np.mean(cross_val_scores), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = dict(n_estimators=list(range(1, 100, 25)),\n",
    "                                    criterion=['gini', 'entropy'],\n",
    "                                    max_depth=[x for x in list(range(2, 5))] + [None],\n",
    "                                    min_samples_split=[x for x in list(range(2, 4))],\n",
    "                                    min_samples_leaf=[x for x in list(range(2, 4))],\n",
    "                                    max_features=[None] + [i * .05 for i in list(range(2, 4))],\n",
    "                                    max_leaf_nodes=list(range(2, 10)) + [None],\n",
    "                                    min_impurity_decrease=[x * .10 for x in list(range(2, 4))],\n",
    "                                    oob_score=[True,False],\n",
    "                                    warm_start=[True, False],\n",
    "                                    class_weight=[None, 'balanced'],\n",
    "                                    max_samples=[None],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:612: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:612: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:612: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:612: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': np.float64(0.5), 'validate': np.float64(0.5)}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo = RandomForestClassifier()\n",
    "modelo, best_estimator, score, params = entrenar(param, modelo, Xt, yt)\n",
    "metricas(Xt,Xv,yt,yv,modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': np.float64(0.926), 'validate': np.float64(0.807)}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parámetros para Random Forest\n",
    "param_rf_corrected = {\n",
    "    'n_estimators': list(range(50, 200, 50)),\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'], \n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "\n",
    "modelo_rf_corrected = RandomForestClassifier()\n",
    "modelo_rf_corrected, best_estimator_rf_corrected, score_rf_corrected, params_rf_corrected = entrenar(param_rf_corrected, modelo_rf_corrected, Xt, yt)\n",
    "metrics_rf_corrected = metricas(Xt, Xv, yt, yv, modelo_rf_corrected)\n",
    "metrics_rf_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUITANDO SOBRE AJUSTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': np.float64(0.889), 'validate': np.float64(0.816)}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parámetros para Random Forest\n",
    "param_rf_corrected = {\n",
    "    'n_estimators': list(range(50, 200, 50)),\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'], \n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "\n",
    "modelo_rf_corrected = RandomForestClassifier()\n",
    "modelo_rf_corrected, best_estimator_rf_corrected, score_rf_corrected, params_rf_corrected = entrenar(param_rf_corrected, modelo_rf_corrected, Xt, yt)\n",
    "metrics_rf_corrected = metricas(Xt, Xv, yt, yv, modelo_rf_corrected)\n",
    "metrics_rf_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': np.float64(0.884), 'validate': np.float64(0.819)}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "param_rf_adjusted = {\n",
    "    'n_estimators': [50, 100], \n",
    "    'criterion': ['gini'],  \n",
    "    'max_depth': [10, 15],  \n",
    "    'min_samples_split': [10, 15], \n",
    "    'min_samples_leaf': [4, 6], \n",
    "    'max_features': ['sqrt'],  \n",
    "    'class_weight': [None, 'balanced']  \n",
    "}\n",
    "\n",
    "\n",
    "modelo_rf_adjusted = RandomForestClassifier()\n",
    "modelo_rf_adjusted, best_estimator_rf_adjusted, score_rf_adjusted, params_rf_adjusted = entrenar(param_rf_adjusted, modelo_rf_adjusted, Xt, yt)\n",
    "metrics_rf_adjusted = metricas(Xt, Xv, yt, yv, modelo_rf_adjusted)\n",
    "\n",
    "metrics_rf_adjusted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': np.float64(0.857), 'validate': np.float64(0.811)}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "param_rf_adjusted = {\n",
    "    'n_estimators': [50, 100],  \n",
    "    'criterion': ['gini'], \n",
    "    'max_depth': [8, 10],  \n",
    "    'min_samples_split': [10, 15],  \n",
    "    'min_samples_leaf': [5, 7],  \n",
    "    'max_features': ['sqrt'],  \n",
    "    'class_weight': [None, 'balanced']  \n",
    "}\n",
    "\n",
    "\n",
    "modelo_rf_adjusted = RandomForestClassifier()\n",
    "modelo_rf_adjusted, best_estimator_rf_adjusted, score_rf_adjusted, params_rf_adjusted = entrenar(param_rf_adjusted, modelo_rf_adjusted, Xt, yt)\n",
    "metrics_rf_adjusted = metricas(Xt, Xv, yt, yv, modelo_rf_adjusted)\n",
    "\n",
    "metrics_rf_adjusted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': np.float64(0.853), 'validate': np.float64(0.812)}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "param_rf_adjusted = {\n",
    "    'n_estimators': [60, 80],  \n",
    "    'criterion': ['gini'],  \n",
    "    'max_depth': [8, 10],  \n",
    "    'min_samples_split': [20, 25],  \n",
    "    'min_samples_leaf': [5, 7],  \n",
    "    'max_features': ['sqrt'],  \n",
    "    'class_weight': ['balanced']  \n",
    "}\n",
    "\n",
    "\n",
    "modelo_rf_adjusted = RandomForestClassifier()\n",
    "modelo_rf_adjusted, best_estimator_rf_adjusted, score_rf_adjusted, params_rf_adjusted = entrenar(param_rf_adjusted, modelo_rf_adjusted, Xt, yt)\n",
    "metrics_rf_adjusted = metricas(Xt, Xv, yt, yv, modelo_rf_adjusted)\n",
    "\n",
    "metrics_rf_adjusted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': np.float64(0.822), 'validate': np.float64(0.808)}"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "param_rf_adjusted = {\n",
    "    'n_estimators': [40, 100], \n",
    "    'criterion': ['gini'],  \n",
    "    'max_depth': [5, 7],  \n",
    "    'min_samples_split': [10, 15], \n",
    "    'min_samples_leaf': [5, 7],  \n",
    "    'max_features': ['sqrt'],  \n",
    "    'class_weight': ['balanced'] \n",
    "}\n",
    "\n",
    "\n",
    "modelo_rf_adjusted = RandomForestClassifier()\n",
    "modelo_rf_adjusted, best_estimator_rf_adjusted, score_rf_adjusted, params_rf_adjusted = entrenar(param_rf_adjusted, modelo_rf_adjusted, Xt, yt)\n",
    "metrics_rf_adjusted = metricas(Xt, Xv, yt, yv, modelo_rf_adjusted)\n",
    "\n",
    "metrics_rf_adjusted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en ./randomforest.pkl\n"
     ]
    }
   ],
   "source": [
    "filename = './randomforest.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(modelo_mlpc,file)\n",
    "    \n",
    "print(f'Modelo guardado en {filename}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
