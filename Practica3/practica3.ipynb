{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        </script>\n",
       "        <script type=\"module\">import \"https://cdn.plot.ly/plotly-3.0.1.min\"</script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "survived\n",
       "0.0    808\n",
       "1.0    500\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "pd.set_option(\"display.max_columns\",30)\n",
    "pd.set_option(\"display.max_rows\",3000)\n",
    "import matplotlib.pyplot as plt \n",
    "import cufflinks as cf \n",
    "cf.go_offline()\n",
    "import numpy as np\n",
    "\n",
    "### imputacion variables continuas\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "df = pd.read_csv(\"titanic.csv\")\n",
    "\n",
    "df.shape\n",
    "\n",
    "df.survived.value_counts(True)\n",
    "\n",
    "df\n",
    "\n",
    "df.survived.value_counts(True)\n",
    "\n",
    "def completitud_datos_nulos(df):\n",
    "    return df.isnull().sum().sort_values(ascending=False) / df.shape[0]\n",
    "\n",
    "completitud_datos_nulos(df)\n",
    "\n",
    "df = df[(~df[\"survived\"].isnull()) & ~df[\"pclass\"].isnull() & ~df[\"fare\"].isnull() ]\n",
    "\n",
    "df\n",
    "\n",
    "def complete_continuous_variables(df,col,strategy='median'):\n",
    "    X = df[col].copy()\n",
    "    im = SimpleImputer(strategy=strategy)\n",
    "    Xi = pd.DataFrame(im.fit_transform(X),columns=col)\n",
    "    l_ks = []\n",
    "    for v in col:\n",
    "        l_ks.append([v,ks_2samp(X[v].dropna(),Xi[v]).statistic])\n",
    "    ks = pd.DataFrame(l_ks,columns=['feat','ks'])\n",
    "    #print(ks)\n",
    "    print((ks.ks>=0.1).sum())\n",
    "    df[col] = im.transform(df[col].copy())\n",
    "    return df\n",
    "\n",
    "complete_continuous_variables(df,[\"body\"])\n",
    "\n",
    "completitud_datos_nulos(df)\n",
    "\n",
    "varc = list(df.describe())\n",
    "\n",
    "varc = [x for x in varc if x not in 'survived']\n",
    "\n",
    "vard = [x for x in df.columns if x not in varc+['survived']]\n",
    "\n",
    "tgt = 'survived'\n",
    "\n",
    "df[tgt].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pclass       0\n",
      "survived     0\n",
      "name         0\n",
      "sex          0\n",
      "age          0\n",
      "sibsp        0\n",
      "parch        0\n",
      "ticket       0\n",
      "fare         0\n",
      "cabin        0\n",
      "embarked     0\n",
      "boat         0\n",
      "body         0\n",
      "home.dest    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Identificar columnas numéricas y categóricas\n",
    "num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "cat_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Imputar valores nulos en las columnas numéricas (mediana)\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "df[num_cols] = num_imputer.fit_transform(df[num_cols])\n",
    "\n",
    "# Imputar valores nulos en las columnas categóricas (moda)\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])\n",
    "\n",
    "# Verificar que no haya valores nulos después de la imputación\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELO CLASIFICACION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[varc].copy() #TAD \"Tabla Analitica de Datos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[tgt].copy() #variable objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = MinMaxScaler()\n",
    "Xs = pd.DataFrame(sc.fit_transform(X), columns=varc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt, Xv, yt, yv = train_test_split(Xs,y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar(param, modelo, X,y):\n",
    "    grid = RandomizedSearchCV(param_distributions=param,\n",
    "                             n_jobs=-1,\n",
    "                             n_iter=10,\n",
    "                             cv=4,\n",
    "                             estimator=modelo,\n",
    "                             error_score='raise')\n",
    "    grid.fit(X,y)\n",
    "    return grid, grid.best_estimator_, grid.best_score_, grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metricas(Xt, Xv, yt, yv, modelo):\n",
    "    d = {'train':round(roc_auc_score(y_true=yt, y_score=modelo.predict_proba(Xt)[:,1]),3),\n",
    "         'validate':round(roc_auc_score(y_true=yv, y_score=modelo.predict_proba(Xv)[:,1]),3)\n",
    "        }\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REDES NEURONALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xt = scaler.fit_transform(Xt)\n",
    "Xv = scaler.transform(Xv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_mlpc = dict(\n",
    "    hidden_layer_sizes = [(50,), (100,), (150,), (50, 50), (100, 100), (150, 150), (100, 50)],\n",
    "    activation = ['relu', 'tanh'],\n",
    "    solver = ['adam'],\n",
    "    alpha = np.logspace(-5, 0, 20),\n",
    "    learning_rate = ['adaptive'],\n",
    "    learning_rate_init = np.logspace(-4, -1, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': np.float64(0.778), 'validate': np.float64(0.785)}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_mlpc = MLPClassifier(max_iter=2000)  # Mayor número de iteraciones para asegurar convergencia\n",
    "\n",
    "# RandomizedSearchCV para un tuning más eficiente\n",
    "search = RandomizedSearchCV(modelo_mlpc, param_mlpc, n_iter=50, scoring='accuracy', cv=5, random_state=0, n_jobs=-1)\n",
    "search.fit(Xt, yt)\n",
    "\n",
    "best_estimator_mlpc = search.best_estimator_\n",
    "score_mlpc = search.best_score_\n",
    "params_mlpc = search.best_params_\n",
    "\n",
    "# Evaluación del modelo en los datos de validación\n",
    "metricas(Xt, Xv, yt, yv, best_estimator_mlpc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xt = scaler.fit_transform(Xt)\n",
    "Xv = scaler.transform(Xv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_mlpc = dict(hidden_layer_sizes = [(a,b,c,) for a in range(len(varc), len(varc)*2) for b in range(len(varc), len(varc)*2) for c in range(len(varc), len(varc)*2)],\n",
    "             activation = ['identity', 'logistic', 'tanh', 'relu'],\n",
    "             solver = ['lbfgs', 'sgd', 'adam'],\n",
    "             alpha = np.arange(0.0001, 0.001, 0.0001),\n",
    "             learning_rate = ['constant', 'invscaling', 'adaptive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning:\n",
      "\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': np.float64(0.785), 'validate': np.float64(0.789)}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_mlpc = MLPClassifier()\n",
    "modelo_mlpc, best_estimator_mlpc, score_mlpc, params_mlpc = entrenar(param_mlpc, modelo_mlpc, Xt, yt)\n",
    "metricas(Xt,Xv,yt,yv,modelo_mlpc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_mlpc = dict(hidden_layer_sizes = [(a,b,c,) for a in range(len(varc), len(varc)*2) for b in range(len(varc), len(varc)*2) for c in range(len(varc), len(varc)*2)],\n",
    "             activation = ['identity', 'logistic', 'tanh', 'relu'],\n",
    "             solver = ['lbfgs', 'sgd', 'adam'],\n",
    "             alpha = np.arange(0.0001, 0.001, 0.0001),\n",
    "             learning_rate = ['constant', 'invscaling', 'adaptive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': np.float64(0.837), 'validate': np.float64(0.726)}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_mlpc = MLPClassifier(\n",
    "    max_iter=6000,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1\n",
    ")\n",
    "\n",
    "modelo_mlpc, best_estimator_mlpc, score_mlpc, params_mlpc = entrenar(param_mlpc, modelo_mlpc, Xt, yt)\n",
    "metricas(Xt,Xv,yt,yv,modelo_mlpc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trabajar despues del sobre ajuste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xt = scaler.fit_transform(Xt)\n",
    "Xv = scaler.transform(Xv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_mlpc = dict(hidden_layer_sizes = [(a,) for a in range(len(varc), len(varc)+5)],\n",
    "             activation = ['identity', 'logistic', 'tanh', 'relu'],\n",
    "             solver = ['lbfgs', 'sgd', 'adam'],\n",
    "             alpha = np.logspace(-4, 0, 10),\n",
    "             learning_rate = ['constant', 'invscaling', 'adaptive'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': np.float64(0.807), 'validate': np.float64(0.802)}"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_mlpc = MLPClassifier(\n",
    "    max_iter = 5000,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1\n",
    ")\n",
    "\n",
    "modelo_mlpc, best_estimator_mlpc, score_mlpc, params_mlpc = entrenar(param_mlpc, modelo_mlpc, Xt, yt)\n",
    "metricas(Xt,Xv,yt,yv,modelo_mlpc)         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en ./modelo_red_neuronal.pkl\n"
     ]
    }
   ],
   "source": [
    "filename = './modelo_red_neuronal.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(modelo_mlpc,file)\n",
    "    \n",
    "print(f'Modelo guardado en {filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NUEVO MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xt = scaler.fit_transform(Xt)\n",
    "Xv = scaler.transform(Xv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_mlpc = dict(hidden_layer_sizes = [(a,) for a in range(len(varc), len(varc)+5)],\n",
    "             activation = ['identity', 'logistic', 'tanh', 'relu'],\n",
    "             solver = ['lbfgs', 'sgd', 'adam'],\n",
    "             alpha = np.logspace(-4, 0, 10),\n",
    "             learning_rate = ['constant', 'invscaling', 'adaptive'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': np.float64(0.8), 'validate': np.float64(0.827)}"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_mlpc = MLPClassifier(\n",
    "    max_iter = 5000,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1\n",
    ")\n",
    "\n",
    "modelo_mlpc, best_estimator_mlpc, score_mlpc, params_mlpc = entrenar(param_mlpc, modelo_mlpc, Xt, yt)\n",
    "metricas(Xt,Xv,yt,yv,modelo_mlpc)         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en ./modelo_red_neuronal2.pkl\n"
     ]
    }
   ],
   "source": [
    "filename = './modelo_red_neuronal2.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(modelo_mlpc,file)\n",
    "    \n",
    "print(f'Modelo guardado en {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Escalar los datos\n",
    "scaler = StandardScaler()\n",
    "Xt_scaled = scaler.fit_transform(Xt)\n",
    "Xv_scaled = scaler.transform(Xv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_mlpc = dict(hidden_layer_sizes = [(a,b,c,) for a in range(len(varc), len(varc)*2) for b in range(len(varc), len(varc)*2) for c in range(len(varc), len(varc)*2)],\n",
    "             activation = ['identity', 'logistic', 'tanh', 'relu'],\n",
    "             solver = ['lbfgs', 'sgd', 'adam'],\n",
    "             alpha = np.arange(0.00001, 0.001, 0.00001),\n",
    "             learning_rate = ['constant', 'invscaling', 'adaptive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': np.float64(0.826), 'validate': np.float64(0.808)}"
      ]
     },
     "execution_count": 677,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_mlpc = MLPClassifier(\n",
    "    max_iter = 10000,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1\n",
    ")\n",
    "\n",
    "modelo_mlpc, best_estimator_mlpc, score_mlpc, params_mlpc = entrenar(param_mlpc, modelo_mlpc, Xt, yt)\n",
    "metricas(Xt,Xv,yt,yv,modelo_mlpc)         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en ./modelo_red_neuronal3.pkl\n"
     ]
    }
   ],
   "source": [
    "filename = './modelo_red_neuronal3.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(modelo_mlpc,file)\n",
    "    \n",
    "print(f'Modelo guardado en {filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "otro modelo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NUEVO MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes de SMOTE: survived\n",
      "0    651\n",
      "1    395\n",
      "Name: count, dtype: int64\n",
      "Después de SMOTE: survived\n",
      "1    651\n",
      "0    651\n",
      "Name: count, dtype: int64\n",
      "Métricas finales: {'train': np.float64(0.961), 'validate': np.float64(0.819)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, VotingClassifier, RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Balancear las clases con SMOTE\n",
    "print(\"Antes de SMOTE:\", yt.value_counts())\n",
    "smote = SMOTE(random_state=42)\n",
    "Xt_res, yt_res = smote.fit_resample(Xt, yt)\n",
    "print(\"Después de SMOTE:\", pd.Series(yt_res).value_counts())\n",
    "\n",
    "# Crear modelos individuales\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "mlp = MLPClassifier(max_iter=5000, random_state=42)\n",
    "\n",
    "# Crear modelo ensemblado (Voting Classifier)\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', rf),\n",
    "        ('gb', gb),\n",
    "        ('mlp', mlp)\n",
    "    ],\n",
    "    voting='soft'  # Usa probabilidades para combinar las predicciones\n",
    ")\n",
    "\n",
    "# Entrenar el modelo ensemblado\n",
    "ensemble.fit(Xt_res, yt_res)\n",
    "\n",
    "# Evaluar las métricas en los conjuntos de entrenamiento y validación\n",
    "metrics = {\n",
    "    'train': round(roc_auc_score(yt_res, ensemble.predict_proba(Xt_res)[:, 1]), 3),\n",
    "    'validate': round(roc_auc_score(yv, ensemble.predict_proba(Xv)[:, 1]), 3)\n",
    "}\n",
    "\n",
    "print(\"Métricas finales:\", metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes de SMOTE: survived\n",
      "0    651\n",
      "1    395\n",
      "Name: count, dtype: int64\n",
      "Después de SMOTE: survived\n",
      "1    651\n",
      "0    651\n",
      "Name: count, dtype: int64\n",
      "Métricas en AUC:\n",
      "Entrenamiento (train): 0.961\n",
      "Validación (validate): 0.821\n",
      "\n",
      "Métricas en F1-Score:\n",
      "Entrenamiento (train): 0.888\n",
      "Validación (validate): 0.682\n",
      "\n",
      "⚠️ Alerta: Puede haber indicios de sobreajuste. Ajusta los hiperparámetros.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, VotingClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Aplicar SMOTE para balancear clases\n",
    "print(\"Antes de SMOTE:\", yt.value_counts())\n",
    "smote = SMOTE(random_state=42)\n",
    "Xt_res, yt_res = smote.fit_resample(Xt, yt)\n",
    "print(\"Después de SMOTE:\", pd.Series(yt_res).value_counts())\n",
    "\n",
    "# Modelos individuales para el ensamble\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "mlp = MLPClassifier(max_iter=5000, random_state=42, early_stopping=True, validation_fraction=0.1)\n",
    "\n",
    "# Ensamble con VotingClassifier\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', rf),\n",
    "        ('gb', gb),\n",
    "        ('mlp', mlp)\n",
    "    ],\n",
    "    voting='soft'  # Combinación basada en probabilidades\n",
    ")\n",
    "\n",
    "# Entrenar el modelo ensamble con los datos balanceados\n",
    "ensemble.fit(Xt_res, yt_res)\n",
    "\n",
    "# Evaluar el desempeño en conjunto de entrenamiento y validación\n",
    "train_auc = roc_auc_score(yt_res, ensemble.predict_proba(Xt_res)[:, 1])\n",
    "validate_auc = roc_auc_score(yv, ensemble.predict_proba(Xv)[:, 1])\n",
    "\n",
    "train_f1 = f1_score(yt_res, ensemble.predict(Xt_res))\n",
    "validate_f1 = f1_score(yv, ensemble.predict(Xv))\n",
    "\n",
    "# Imprimir métricas\n",
    "print(\"Métricas en AUC:\")\n",
    "print(\"Entrenamiento (train):\", round(train_auc, 3))\n",
    "print(\"Validación (validate):\", round(validate_auc, 3))\n",
    "\n",
    "print(\"\\nMétricas en F1-Score:\")\n",
    "print(\"Entrenamiento (train):\", round(train_f1, 3))\n",
    "print(\"Validación (validate):\", round(validate_f1, 3))\n",
    "\n",
    "# Comprobación de sobreajuste\n",
    "if (train_auc - validate_auc) > 0.05:\n",
    "    print(\"\\n⚠️ Alerta: Puede haber indicios de sobreajuste. Ajusta los hiperparámetros.\")\n",
    "else:\n",
    "    print(\"\\n✅ No se detectó sobreajuste significativo.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento (train): 0.83397\n",
      "Validación (validate): 0.81729\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(\n",
    "    max_iter=5000,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    alpha=0.01\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.05, random_state=42)\n",
    "\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', rf),\n",
    "        ('gb', gb),\n",
    "        ('mlp', mlp)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Entrenar y evaluar\n",
    "ensemble.fit(Xt_res, yt_res)\n",
    "\n",
    "train_auc = roc_auc_score(yt_res, ensemble.predict_proba(Xt_res)[:, 1])\n",
    "validate_auc = roc_auc_score(yv, ensemble.predict_proba(Xv)[:, 1])\n",
    "\n",
    "print(\"Entrenamiento (train):\", round(train_auc, 5))\n",
    "print(\"Validación (validate):\", round(validate_auc, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento (train): 0.83\n",
      "Validación (validate): 0.816\n",
      "Promedio de AUC en validación cruzada: 0.79\n"
     ]
    }
   ],
   "source": [
    "# Modelos ajustados\n",
    "mlp = MLPClassifier(\n",
    "    max_iter=5000,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    alpha=0.1,\n",
    "    hidden_layer_sizes=(10,)\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "\n",
    "gb = GradientBoostingClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Opcional: Cambiar gb por xgb si tienes XGBoost\n",
    "# from xgboost import XGBClassifier\n",
    "# gb = XGBClassifier(...)\n",
    "\n",
    "# Ensamble ajustado\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', rf),\n",
    "        ('gb', gb),\n",
    "        ('mlp', mlp)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Entrenar el ensamble\n",
    "ensemble.fit(Xt_res, yt_res)\n",
    "\n",
    "# Métricas de AUC\n",
    "train_auc = roc_auc_score(yt_res, ensemble.predict_proba(Xt_res)[:, 1])\n",
    "validate_auc = roc_auc_score(yv, ensemble.predict_proba(Xv)[:, 1])\n",
    "\n",
    "print(\"Entrenamiento (train):\", round(train_auc, 3))\n",
    "print(\"Validación (validate):\", round(validate_auc, 3))\n",
    "\n",
    "# Validación cruzada\n",
    "cross_val_scores = cross_val_score(ensemble, Xt_res, yt_res, scoring='roc_auc', cv=5)\n",
    "print(\"Promedio de AUC en validación cruzada:\", round(np.mean(cross_val_scores), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MEJORANDO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento (train): 0.849\n",
      "Validación (validate): 0.846\n",
      "Promedio de AUC en validación cruzada: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Importar librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Configuración inicial\n",
    "pd.set_option(\"display.max_columns\", 30)\n",
    "pd.set_option(\"display.max_rows\", 3000)\n",
    "\n",
    "# Cargar datos\n",
    "df = pd.read_csv(\"titanic.csv\")\n",
    "\n",
    "# Función para verificar datos nulos\n",
    "def completitud_datos_nulos(df):\n",
    "    return df.isnull().sum().sort_values(ascending=False) / df.shape[0]\n",
    "\n",
    "# Filtrar datos nulos en columnas clave\n",
    "df = df[(~df[\"survived\"].isnull()) & ~df[\"pclass\"].isnull() & ~df[\"fare\"].isnull()]\n",
    "\n",
    "# Imputar valores numéricos y categóricos\n",
    "num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "cat_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "df[num_cols] = num_imputer.fit_transform(df[num_cols])\n",
    "\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])\n",
    "\n",
    "# Variables objetivo y predictoras\n",
    "tgt = 'survived'\n",
    "varc = list(df.describe())\n",
    "varc = [x for x in varc if x != tgt]\n",
    "vard = [x for x in df.columns if x not in varc + [tgt]]\n",
    "\n",
    "X = df[varc].copy()\n",
    "y = df[tgt].astype(int).copy()\n",
    "\n",
    "# Escalado de datos\n",
    "sc = MinMaxScaler()\n",
    "Xs = pd.DataFrame(sc.fit_transform(X), columns=varc)\n",
    "\n",
    "# División de datos\n",
    "Xt, Xv, yt, yv = train_test_split(Xs, y, train_size=0.8, random_state=42)\n",
    "\n",
    "# Balanceo de datos con SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "Xt_res, yt_res = smote.fit_resample(Xt, yt)\n",
    "\n",
    "# Modelos ajustados\n",
    "mlp = MLPClassifier(\n",
    "    max_iter=5000,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    alpha=0.01,\n",
    "    hidden_layer_sizes=(50, 30, 10)\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=150, max_depth=6, random_state=42)\n",
    "\n",
    "gb = GradientBoostingClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Ensamble ajustado\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', rf),\n",
    "        ('gb', gb),\n",
    "        ('mlp', mlp)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Entrenar el ensamble\n",
    "ensemble.fit(Xt_res, yt_res)\n",
    "\n",
    "# Métricas de AUC\n",
    "train_auc = roc_auc_score(yt_res, ensemble.predict_proba(Xt_res)[:, 1])\n",
    "validate_auc = roc_auc_score(yv, ensemble.predict_proba(Xv)[:, 1])\n",
    "\n",
    "print(\"Entrenamiento (train):\", round(train_auc, 3))\n",
    "print(\"Validación (validate):\", round(validate_auc, 3))\n",
    "\n",
    "# Validación cruzada\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cross_val_scores = cross_val_score(ensemble, Xt_res, yt_res, scoring='roc_auc', cv=cv)\n",
    "print(\"Promedio de AUC en validación cruzada:\", round(np.mean(cross_val_scores), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "otro modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADA BOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost\n",
    "param_adab = {\n",
    "    'n_estimators': range(50, 200),\n",
    "    'learning_rate': np.arange(0.01, 0.2, 0.01),\n",
    "    'algorithm': ['SAMME.R']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_adab = AdaBoostClassifier(random_state=42)\n",
    "modelo_adab, best_estimator_adab, score_adab, params_adab = entrenar(param_adab, modelo_adab, Xt, yt)\n",
    "metricas_adab = metricas(Xt, Xv, yt, yv, modelo_adab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'algorithm' parameter of AdaBoostClassifier must be a str among {'SAMME'}. Got 'SAMME.R' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31m_RemoteTraceback\u001b[39m                          Traceback (most recent call last)",
      "\u001b[31m_RemoteTraceback\u001b[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/joblib/parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/base.py\", line 1382, in wrapper\n    estimator._validate_params()\n  File \"/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/base.py\", line 436, in _validate_params\n    validate_parameter_constraints(\n  File \"/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'algorithm' parameter of AdaBoostClassifier must be a str among {'SAMME'}. Got 'SAMME.R' instead.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mInvalidParameterError\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Entrenar AdaBoost\u001b[39;00m\n\u001b[32m      9\u001b[39m modelo_adab = AdaBoostClassifier()\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m modelo_adab, best_estimator_adab, score_adab, params_adab = \u001b[43mentrenar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_adab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelo_adab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m metrics_adab = metricas(Xt, Xv, yt, yv, modelo_adab)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mentrenar\u001b[39m\u001b[34m(param, modelo, X, y)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mentrenar\u001b[39m(param, modelo, X,y):\n\u001b[32m      2\u001b[39m     grid = RandomizedSearchCV(param_distributions=param,\n\u001b[32m      3\u001b[39m                              n_jobs=-\u001b[32m1\u001b[39m,\n\u001b[32m      4\u001b[39m                              n_iter=\u001b[32m10\u001b[39m,\n\u001b[32m      5\u001b[39m                              cv=\u001b[32m4\u001b[39m,\n\u001b[32m      6\u001b[39m                              estimator=modelo,\n\u001b[32m      7\u001b[39m                              error_score=\u001b[33m'\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[43mgrid\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m grid, grid.best_estimator_, grid.best_score_, grid.best_params_\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1024\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1018\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1019\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1020\u001b[39m     )\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1027\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1028\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1951\u001b[39m, in \u001b[36mRandomizedSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1950\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1951\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1952\u001b[39m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1953\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[32m   1954\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1955\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:970\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    963\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    964\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    965\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    966\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    967\u001b[39m         )\n\u001b[32m    968\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m    989\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    990\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    993\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/utils/parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2001\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2002\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2003\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2004\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2005\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2007\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1647\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1649\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1650\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1652\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1653\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1654\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1655\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1656\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/joblib/parallel.py:1754\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._wait_retrieval():\n\u001b[32m   1748\u001b[39m \n\u001b[32m   1749\u001b[39m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[32m   1751\u001b[39m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[32m   1752\u001b[39m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[32m   1753\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._aborting:\n\u001b[32m-> \u001b[39m\u001b[32m1754\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1755\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1757\u001b[39m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[32m   1758\u001b[39m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/joblib/parallel.py:1789\u001b[39m, in \u001b[36mParallel._raise_error_fast\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1785\u001b[39m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[32m   1786\u001b[39m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[32m   1787\u001b[39m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[32m   1788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1789\u001b[39m     \u001b[43merror_job\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/joblib/parallel.py:745\u001b[39m, in \u001b[36mBatchCompletionCallBack.get_result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    739\u001b[39m backend = \u001b[38;5;28mself\u001b[39m.parallel._backend\n\u001b[32m    741\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend.supports_retrieve_callback:\n\u001b[32m    742\u001b[39m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[32m    743\u001b[39m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[32m    744\u001b[39m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    747\u001b[39m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/joblib/parallel.py:763\u001b[39m, in \u001b[36mBatchCompletionCallBack._return_or_raise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    762\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.status == TASK_ERROR:\n\u001b[32m--> \u001b[39m\u001b[32m763\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    764\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n\u001b[32m    765\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mInvalidParameterError\u001b[39m: The 'algorithm' parameter of AdaBoostClassifier must be a str among {'SAMME'}. Got 'SAMME.R' instead."
     ]
    }
   ],
   "source": [
    "# Parámetros para AdaBoost\n",
    "param_adab = {\n",
    "    'n_estimators': range(50, 200, 50),\n",
    "    'learning_rate': np.arange(0.01, 1, 0.1),\n",
    "    'algorithm': ['SAMME.R']\n",
    "}\n",
    "\n",
    "# Entrenar AdaBoost\n",
    "modelo_adab = AdaBoostClassifier()\n",
    "modelo_adab, best_estimator_adab, score_adab, params_adab = entrenar(param_adab, modelo_adab, Xt, yt)\n",
    "metrics_adab = metricas(Xt, Xv, yt, yv, modelo_adab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = dict(n_estimators=list(range(1, 100, 25)),\n",
    "                                    criterion=['gini', 'entropy'],\n",
    "                                    max_depth=[x for x in list(range(2, 5))] + [None],\n",
    "                                    min_samples_split=[x for x in list(range(2, 4))],\n",
    "                                    min_samples_leaf=[x for x in list(range(2, 4))],\n",
    "                                    max_features=[None] + [i * .05 for i in list(range(2, 4))],\n",
    "                                    max_leaf_nodes=list(range(2, 10)) + [None],\n",
    "                                    min_impurity_decrease=[x * .10 for x in list(range(2, 4))],\n",
    "                                    oob_score=[True,False],\n",
    "                                    warm_start=[True, False],\n",
    "                                    class_weight=[None, 'balanced'],\n",
    "                                    max_samples=[None],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:612: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:612: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:612: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:612: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n",
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': np.float64(0.5), 'validate': np.float64(0.5)}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo = RandomForestClassifier()\n",
    "modelo, best_estimator, score, params = entrenar(param, modelo, Xt, yt)\n",
    "metricas(Xt,Xv,yt,yv,modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': np.float64(0.926), 'validate': np.float64(0.807)}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parámetros para Random Forest\n",
    "param_rf_corrected = {\n",
    "    'n_estimators': list(range(50, 200, 50)),\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'],  # Corregido\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "# Entrenar Random Forest con parámetros corregidos\n",
    "modelo_rf_corrected = RandomForestClassifier()\n",
    "modelo_rf_corrected, best_estimator_rf_corrected, score_rf_corrected, params_rf_corrected = entrenar(param_rf_corrected, modelo_rf_corrected, Xt, yt)\n",
    "metrics_rf_corrected = metricas(Xt, Xv, yt, yv, modelo_rf_corrected)\n",
    "metrics_rf_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUITANDO SOBRE AJUSTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': np.float64(0.889), 'validate': np.float64(0.816)}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parámetros para Random Forest\n",
    "param_rf_corrected = {\n",
    "    'n_estimators': list(range(50, 200, 50)),\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'],  # Corregido\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "# Entrenar Random Forest con parámetros corregidos\n",
    "modelo_rf_corrected = RandomForestClassifier()\n",
    "modelo_rf_corrected, best_estimator_rf_corrected, score_rf_corrected, params_rf_corrected = entrenar(param_rf_corrected, modelo_rf_corrected, Xt, yt)\n",
    "metrics_rf_corrected = metricas(Xt, Xv, yt, yv, modelo_rf_corrected)\n",
    "metrics_rf_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': np.float64(0.884), 'validate': np.float64(0.819)}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parámetros ajustados para Random Forest\n",
    "param_rf_adjusted = {\n",
    "    'n_estimators': [50, 100],  # Reducir el número de árboles\n",
    "    'criterion': ['gini'],  # Usar solo 'gini' para simplificar\n",
    "    'max_depth': [10, 15],  # Reducir la profundidad máxima\n",
    "    'min_samples_split': [10, 15],  # Aumentar el número mínimo de muestras para dividir\n",
    "    'min_samples_leaf': [4, 6],  # Aumentar el número mínimo de muestras en una hoja\n",
    "    'max_features': ['sqrt'],  # Usar solo 'sqrt' para simplificar\n",
    "    'class_weight': [None, 'balanced']  # Considerar el balanceo de clases\n",
    "}\n",
    "\n",
    "# Entrenar Random Forest con parámetros ajustados\n",
    "modelo_rf_adjusted = RandomForestClassifier()\n",
    "modelo_rf_adjusted, best_estimator_rf_adjusted, score_rf_adjusted, params_rf_adjusted = entrenar(param_rf_adjusted, modelo_rf_adjusted, Xt, yt)\n",
    "metrics_rf_adjusted = metricas(Xt, Xv, yt, yv, modelo_rf_adjusted)\n",
    "\n",
    "metrics_rf_adjusted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': np.float64(0.857), 'validate': np.float64(0.811)}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parámetros ajustados para Random Forest\n",
    "param_rf_adjusted = {\n",
    "    'n_estimators': [50, 100],  # Reducir el número de árboles\n",
    "    'criterion': ['gini'],  # Usar solo 'gini' para simplificar\n",
    "    'max_depth': [8, 10],  # Reducir la profundidad máxima\n",
    "    'min_samples_split': [10, 15],  # Aumentar el número mínimo de muestras para dividir\n",
    "    'min_samples_leaf': [5, 7],  # Aumentar el número mínimo de muestras en una hoja\n",
    "    'max_features': ['sqrt'],  # Usar solo 'sqrt' para simplificar\n",
    "    'class_weight': [None, 'balanced']  # Considerar el balanceo de clases\n",
    "}\n",
    "\n",
    "# Entrenar Random Forest con parámetros ajustados\n",
    "modelo_rf_adjusted = RandomForestClassifier()\n",
    "modelo_rf_adjusted, best_estimator_rf_adjusted, score_rf_adjusted, params_rf_adjusted = entrenar(param_rf_adjusted, modelo_rf_adjusted, Xt, yt)\n",
    "metrics_rf_adjusted = metricas(Xt, Xv, yt, yv, modelo_rf_adjusted)\n",
    "\n",
    "metrics_rf_adjusted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': np.float64(0.853), 'validate': np.float64(0.812)}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parámetros ajustados para Random Forest\n",
    "param_rf_adjusted = {\n",
    "    'n_estimators': [60, 80],  # Reducir el número de árboles\n",
    "    'criterion': ['gini'],  # Usar solo 'gini' para simplificar\n",
    "    'max_depth': [8, 10],  # Reducir la profundidad máxima\n",
    "    'min_samples_split': [20, 25],  # Aumentar el número mínimo de muestras para dividir\n",
    "    'min_samples_leaf': [5, 7],  # Aumentar el número mínimo de muestras en una hoja\n",
    "    'max_features': ['sqrt'],  # Usar solo 'sqrt' para simplificar\n",
    "    'class_weight': ['balanced']  # Considerar el balanceo de clases\n",
    "}\n",
    "\n",
    "# Entrenar Random Forest con parámetros ajustados\n",
    "modelo_rf_adjusted = RandomForestClassifier()\n",
    "modelo_rf_adjusted, best_estimator_rf_adjusted, score_rf_adjusted, params_rf_adjusted = entrenar(param_rf_adjusted, modelo_rf_adjusted, Xt, yt)\n",
    "metrics_rf_adjusted = metricas(Xt, Xv, yt, yv, modelo_rf_adjusted)\n",
    "\n",
    "metrics_rf_adjusted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': np.float64(0.867), 'validate': np.float64(0.816)}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parámetros ajustados para Random Forest\n",
    "param_rf_adjusted = {\n",
    "    'n_estimators': [75, 100],  # Aumentar ligeramente el número de árboles\n",
    "    'criterion': ['gini'],  # Usar solo 'gini' para simplificar\n",
    "    'max_depth': [9, 10],  # Probar con profundidad intermedia\n",
    "    'min_samples_split': [10, 15],  # Reducir ligeramente para encontrar equilibrio\n",
    "    'min_samples_leaf': [4, 5],  # Reducir ligeramente para encontrar equilibrio\n",
    "    'max_features': ['sqrt'],  # Usar solo 'sqrt' para simplificar\n",
    "    'class_weight': ['balanced']  # Considerar el balanceo de clases\n",
    "}\n",
    "\n",
    "# Entrenar Random Forest con parámetros ajustados\n",
    "modelo_rf_adjusted = RandomForestClassifier()\n",
    "modelo_rf_adjusted, best_estimator_rf_adjusted, score_rf_adjusted, params_rf_adjusted = entrenar(param_rf_adjusted, modelo_rf_adjusted, Xt, yt)\n",
    "metrics_rf_adjusted = metricas(Xt, Xv, yt, yv, modelo_rf_adjusted)\n",
    "\n",
    "metrics_rf_adjusted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': np.float64(0.847), 'validate': np.float64(0.79)}"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Parámetros para Random Forest\n",
    "param_rf_adjusted = {\n",
    "    'n_estimators': [75, 100],\n",
    "    'criterion': ['gini'],\n",
    "    'max_depth': [8, 9],  # Reducir la profundidad máxima\n",
    "    'min_samples_split': [8, 10],  # Ajustar para encontrar equilibrio\n",
    "    'min_samples_leaf': [3, 4],  # Ajustar para encontrar equilibrio\n",
    "    'max_features': ['sqrt'],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "# Entrenar Random Forest con parámetros ajustados\n",
    "modelo_rf_adjusted = RandomForestClassifier()\n",
    "modelo_rf_adjusted, best_estimator_rf_adjusted, score_rf_adjusted, params_rf_adjusted = entrenar(param_rf_adjusted, modelo_rf_adjusted, Xt, yt)\n",
    "\n",
    "# Parámetros para la red neuronal\n",
    "param_mlpc = {\n",
    "    'hidden_layer_sizes': [(a,) for a in range(len(varc), len(varc)+5)],\n",
    "    'activation': ['relu'],  # Usar solo 'relu' para simplificar\n",
    "    'solver': ['adam'],  # Usar solo 'adam' para simplificar\n",
    "    'alpha': np.logspace(-4, -2, 5),  # Reducir el rango de alpha\n",
    "    'learning_rate': ['adaptive']\n",
    "}\n",
    "\n",
    "# Entrenar la red neuronal\n",
    "modelo_mlpc = MLPClassifier(max_iter=8000, random_state=42, early_stopping=True, validation_fraction=0.01)\n",
    "modelo_mlpc, best_estimator_mlpc, score_mlpc, params_mlpc = entrenar(param_mlpc, modelo_mlpc, Xt, yt)\n",
    "\n",
    "# Crear el ensamblaje usando Stacking\n",
    "meta_model = SVC(kernel='linear', probability=True)  # Usar SVM como meta-modelo\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('mlpc', best_estimator_mlpc),\n",
    "        ('rf', best_estimator_rf_adjusted)\n",
    "    ],\n",
    "    final_estimator=meta_model,\n",
    "    cv=5  # Usar validación cruzada para entrenar el meta-modelo\n",
    ")\n",
    "\n",
    "# Entrenar el modelo de stacking\n",
    "stacking_model.fit(Xt, yt)\n",
    "\n",
    "# Evaluar el modelo de stacking\n",
    "metrics_stacking = metricas(Xt, Xv, yt, yv, stacking_model)\n",
    "metrics_stacking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANALISIS DISCRIMINANTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrección del parámetro de búsqueda para Análisis Discriminante\n",
    "param_lda_corrected = {\n",
    "    'solver': ['lsqr'],  # Solo usar 'lsqr' para shrinkage\n",
    "    'shrinkage': [None, 'auto']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javier/Documentos/Mineria De Datos/Repositorio/DMUNAM2025/venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:317: UserWarning:\n",
      "\n",
      "The total space of parameters 2 is smaller than n_iter=10. Running 2 iterations. For exhaustive searches, use GridSearchCV.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': np.float64(0.722), 'validate': np.float64(0.703)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenar el modelo de Análisis Discriminante con los parámetros corregidos\n",
    "modelo_lda_corrected = LinearDiscriminantAnalysis()\n",
    "modelo_lda_corrected, best_estimator_lda_corrected, score_lda_corrected, params_lda_corrected = entrenar(param_lda_corrected, modelo_lda_corrected, Xt, yt)\n",
    "metricas_lda_corrected = metricas(Xt, Xv, yt, yv, modelo_lda_corrected)\n",
    "metricas_lda_corrected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': np.float64(0.71), 'validate': np.float64(0.724)}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Parámetros para Análisis Discriminante\n",
    "param_lda = {\n",
    "    'solver': ['lsqr', 'eigen'],  # 'svd' no soporta 'shrinkage'\n",
    "    'shrinkage': [None, 'auto', 0.1, 0.5, 0.9]  # Ajustar shrinkage\n",
    "}\n",
    "\n",
    "# Entrenar Análisis Discriminante\n",
    "modelo_lda = LinearDiscriminantAnalysis()\n",
    "modelo_lda, best_estimator_lda, score_lda, params_lda = entrenar(param_lda, modelo_lda, Xt, yt)\n",
    "metrics_lda = metricas(Xt, Xv, yt, yv, modelo_lda)\n",
    "\n",
    "# Resultados\n",
    "metrics_lda\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
