{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1288,
   "id": "0d94f4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1289,
   "id": "82ec57da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "# pd.set_option(\"display.max_columns\",30)\n",
    "# pd.set_option(\"display.max_rows\",3000)\n",
    "import matplotlib.pyplot as plt \n",
    "import cufflinks as cf \n",
    "# cf.go_offline()\n",
    "import numpy as np\n",
    "\n",
    "### imputacion variables continuas\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import ks_2samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1290,
   "id": "58bc5e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\jorge\\OneDrive\\Documentos\\Documentos\\UNAM\\Minería de Datos\\P3\\DMUNAM2025\\Practica_3\\Dataset\\titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1291,
   "id": "6738e72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1292,
   "id": "5840c48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.survived.value_counts(True)\n",
    "\n",
    "\n",
    "# Podemos obervar gracias a nuestra funcion que tenemos 61% de personas que no sobrevivieron vs 38% que han sobrevivido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1293,
   "id": "7f1194e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1294,
   "id": "b04c6c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def completitud_datos_nulos(df):\n",
    "    return df.isnull().sum().sort_values(ascending=False) / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1295,
   "id": "7a8e3153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# completitud_datos_nulos(df)\n",
    "\n",
    "\n",
    "    # Podemos observar que  body, cabin, boat y home.dest tien un porcentaje alto de nuelos, por lo cual podemos eliminarlas dado que no nos aportaran valor a nuestro modelo y si quisieramos llenarlo seria con valores NO CONFIABLES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1296,
   "id": "16515525",
   "metadata": {},
   "outputs": [],
   "source": [
    "columna_e = ['body', 'cabin', 'boat', 'home.dest']\n",
    "\n",
    "df = df.drop(columna_e, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1297,
   "id": "caca5cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# completitud_datos_nulos(df)\n",
    "\n",
    "\n",
    "    # podemos observar que ahora solo tenemos age y embarked con nulos, considerendo su porcentaje podemos imputar datos al modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1298,
   "id": "23cfa3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(~df[\"survived\"].isnull()) & ~df[\"pclass\"].isnull() & ~df[\"fare\"].isnull() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1299,
   "id": "09c55bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1300,
   "id": "4c69b5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos ver que nuestro registros han bajado a 1308 a consderacion de los 1310 que teniamos al inicio, asi mismo disminuyeron las columnas a 10 de las 14 inciales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1301,
   "id": "f1df5e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df\n",
    "\n",
    "\n",
    "    # Podemos observar que ahora nuestro modelo se encuentra mucho mas limpio y con pocos nulos, por lo que vamos a porceder a imputar nuestras variables y asi llenar nuestros huecos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1302,
   "id": "4a866aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_continuous_variables(df,col,strategy='median'):\n",
    "    X = df[col].copy()\n",
    "    im = SimpleImputer(strategy=strategy)\n",
    "    Xi = pd.DataFrame(im.fit_transform(X),columns=col)\n",
    "    l_ks = []\n",
    "    for v in col:\n",
    "        l_ks.append([v, ks_2samp(X[v].dropna(), Xi[v]).statistic])\n",
    "    ks = pd.DataFrame(l_ks,columns=['feat','ks'])\n",
    "    print(ks)\n",
    "    print((ks.ks>=0.1).sum())\n",
    "    #     df[col] = im.transform(df[col].copy())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1303,
   "id": "0f1ff684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# completitud_datos_nulos(df)\n",
    "\n",
    "\n",
    "    # Recordemos que tenemos variables nulas en embarked y age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1304,
   "id": "10fe2e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['age' ]]. copy()\n",
    "im = SimpleImputer(strategy='median')\n",
    "    # df['age'] = im.fit_transform(df[['age']].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1305,
   "id": "9b59d02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La razón por la cual usaremos SimpleImputer con median es porque a mediana es resistente a valores atípicos (outliers), por eso se prefiere cuando los datos pueden estar sesgados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1306,
   "id": "6228583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['embarked'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1307,
   "id": "fede0e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto cuenta la cantidad de veces que aparece cada valor en la columna embarked, incluyendo los valores nulos (NaN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1308,
   "id": "1ff1abd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embarked = df[['embarked']].copy() \n",
    "im = SimpleImputer(strategy='most_frequent') \n",
    "    # df['embarked'] = im.fit_transform(X_embarked).ravel()\n",
    "\n",
    "\n",
    "    # Se rellena con el dato mas frecuente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1309,
   "id": "08563a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1310,
   "id": "c1472089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# completitud_datos_nulos(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1311,
   "id": "27c51f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# completitud_datos_nulos(df)\n",
    "\n",
    "\n",
    "    # Al realizar todo este analsis ya podemos decir que no hay nulos en nuestras columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1312,
   "id": "c489cdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sample(n=30)\n",
    "\n",
    "\n",
    "    # Tomamos una muestra aleatoria de nuestro DataFrame, dado que name es el nombre de la persona sobreviviente o fallecida, podemos eliminarlo dado que no aporta algo a nuestro modelo, asi como el ticket, ya que es una combinacion de numeros y letras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1313,
   "id": "ab4a305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "columna_ee = ['name', 'ticket']\n",
    "\n",
    "df = df.drop(columna_ee, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1314,
   "id": "426a2a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape\n",
    "    # completitud_datos_nulos(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1315,
   "id": "db90709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sample(n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1316,
   "id": "6301a4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sexo = {'male':0, 'female' :1}\n",
    "    # df['sex'] = df['sex'].map(sexo)\n",
    "\n",
    "embarked = {'C':0, 'Q':1, 'S': 2}\n",
    "    # df['embarked' ] = df['embarked' ].map(embarked)\n",
    "\n",
    "\n",
    "    # La columna sex fue convertida a valores numéricos, asignando 0 para 'male' y 1 para 'female'.\n",
    "    # \n",
    "    # La columna embarked, que representa el puerto de embarque, fue codificada como 0 para 'C' (Cherbourg), 1 para 'Q' (Queenstown) y 2 para 'S' (Southampton)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1317,
   "id": "1692c866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sample(n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1318,
   "id": "a428ad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "varc = list(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1319,
   "id": "f4f88f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "varc = [x for x in varc if x not in 'survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1320,
   "id": "347fbc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vard = [x for x in df.columns if x not in varc+['survived']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1321,
   "id": "e936eebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt = 'survived'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1322,
   "id": "993d741c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[tgt].value_counts()\n",
    "\n",
    "\n",
    "    # # Modelacion clasificacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1323,
   "id": "c1001d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1324,
   "id": "d8065ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[varc].copy() #TAD \"Tabla Analitica de Datos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1325,
   "id": "af99ddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[tgt].copy() #variable objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1326,
   "id": "451851d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = MinMaxScaler()\n",
    "Xs = pd.DataFrame(sc.fit_transform(X), columns=varc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1327,
   "id": "a766e390",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1328,
   "id": "d360aea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt, Xv, yt, yv = train_test_split(Xs,y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1329,
   "id": "593d4dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar(param, modelo, X,y):\n",
    "    grid = RandomizedSearchCV(param_distributions=param,\n",
    "    n_jobs=-1,\n",
    "    n_iter=10,\n",
    "    cv=4,\n",
    "    estimator=modelo,\n",
    "    error_score='raise')\n",
    "    #     grid.fit(X,y)\n",
    "    return grid, grid.best_estimator_, grid.best_score_, grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1330,
   "id": "4a73e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1331,
   "id": "358ec6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_mlpc = dict(\n",
    "    hidden_layer_sizes=[(a, b, c) for a in range(len(varc), len(varc)*2)\n",
    "                                  for b in range(len(varc), len(varc)*2)\n",
    "                                  for c in range(len(varc), len(varc)*2)],\n",
    "    activation=['relu', 'tanh'],\n",
    "    solver=['adam'],\n",
    "    alpha=[0.0001, 0.001, 0.01]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1332,
   "id": "4adf2d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_mlpc = MLPClassifier(solver='adam', max_iter=1000, random_state=42)\n",
    "    # modelo_mlpc, best_estimator_mlpc, score_mlpc, params_mlpc = entrenar(param_mlpc, modelo_mlpc, Xt, yt)\n",
    "    # metricas(Xt,Xv,yt,yv,modelo_mlpc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1333,
   "id": "d269fd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1334,
   "id": "05132f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en ./Najera_redneural.pkl\n"
     ]
    }
   ],
   "source": [
    "filename = './Najera_redneural.pkl'\n",
    "    # with open(filename, 'wb') as file:\n",
    "    #     pickle.dump(modelo_mlpc,file)\n",
    "print(f'Modelo guardado en {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1335,
   "id": "70a21f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1336,
   "id": "b76d8a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_mlpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1337,
   "id": "ce5f6c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_mlpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1338,
   "id": "5ab1b0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_estimator_mlpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1339,
   "id": "dac8ae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo_mlpc\n",
    "            # # Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1340,
   "id": "481e305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = dict(n_estimators = range(2,10),\n",
    "    max_depeth = range(2,6),\n",
    "    max_features = range(2,len(varc)),\n",
    "    criterion = ['gini', 'entropy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1341,
   "id": "6506f3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = dict(n_estimators=list(range(1, 100, 25)),\n",
    "    criterion=['gini', 'entropy'],\n",
    "    max_depth=[x for x in list(range(2, 5))] + [None],\n",
    "    min_samples_split=[x for x in list(range(2, 4))],\n",
    "    min_samples_leaf=[x for x in list(range(2, 4))],\n",
    "    max_features=[None] + [i * .05 for i in list(range(2, 4))],\n",
    "    max_leaf_nodes=list(range(2, 10)) + [None],\n",
    "    min_impurity_decrease=[x * .10 for x in list(range(2, 4))],\n",
    "    oob_score=[True,False],\n",
    "    warm_start=[True, False],\n",
    "    class_weight=[None, 'balanced'],\n",
    "    max_samples=[None],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1342,
   "id": "44d7424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = RandomForestClassifier()\n",
    "    # modelo, best_estimator, score, params = entrenar(param, modelo, Xt, yt)\n",
    "    # metricas(Xt,Xv,yt,yv,modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1343,
   "id": "cfe8716b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en ./Najera_randomforest.pkl\n"
     ]
    }
   ],
   "source": [
    "filename = './Najera_randomforest.pkl'\n",
    "    # with open(filename, 'wb') as file:\n",
    "    #     pickle.dump(modelo_mlpc, file)\n",
    "print(f'Modelo guardado en {filename}')\n",
    "            # # Ada Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1344,
   "id": "b7c7ff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1345,
   "id": "17524359",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_adab = dict(n_estimators = range(2,10),\n",
    "    learning_rate = np.arange(0.1,1,0.1),\n",
    "    algorithm = ['SAMME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1346,
   "id": "3013393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_adab = AdaBoostClassifier()\n",
    "    # modelo_adab, best_estimator, score, params = entrenar(param_adab, modelo_adab, Xt, yt)\n",
    "    # metricas(Xt,Xv,yt,yv,modelo_adab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1347,
   "id": "05d455bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en ./Najera_adaboost.pkl\n"
     ]
    }
   ],
   "source": [
    "filename = './Najera_adaboost.pkl'\n",
    "    # with open(filename, 'wb') as file:\n",
    "    #     pickle.dump(modelo_mlpc, file)\n",
    "print(f'Modelo guardado en {filename}')\n",
    "            # # Analisis Discriminante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1348,
   "id": "08023daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_lda = dict(solver = ['svd', 'lsqr', 'eigen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1349,
   "id": "45a22546",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = LinearDiscriminantAnalysis()\n",
    "    # modelo, best_estimator, score, params = entrenar(param_lda, modelo, Xt, yt)\n",
    "    # metricas(Xt,Xv,yt,yv,modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1350,
   "id": "43527b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en ./Najera_analisisdiscriminante.pkl\n"
     ]
    }
   ],
   "source": [
    "filename = './Najera_analisisdiscriminante.pkl'\n",
    "    # with open(filename, 'wb') as file:\n",
    "    #     pickle.dump(modelo_mlpc, file)\n",
    "print(f'Modelo guardado en {filename}')\n",
    "            # # Maquina Vector Soporte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1351,
   "id": "7a11b06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_svc = dict(C = np.arange(0,2,0.1),\n",
    "    kernel = ['linear','poly','rbf','sigmoid'],\n",
    "    degree = range(2,6),\n",
    "    gamma = ['scale','auto'], \n",
    "    probability = [True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1352,
   "id": "b52c6c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo_svc, best_estimator, score, params = entrenar(param_svc, modelo_svc, Xt, yt)\n",
    "    # metricas(Xt,Xv,yt,yv,modelo_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1353,
   "id": "f01f2042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en ./Najera_maquinavectorsoporte.pkl\n"
     ]
    }
   ],
   "source": [
    "filename = './Najera_maquinavectorsoporte.pkl'\n",
    "    # with open(filename, 'wb') as file:\n",
    "    #     pickle.dump(modelo_mlpc, file)\n",
    "print(f'Modelo guardado en {filename}')\n",
    "            # Conclusiones de los modelos\n",
    "    # \n",
    "    # Red Nuronal: 85.8 % \n",
    "    # Ramdom Forest: 77.7%\n",
    "    # Ada Bost:83.4%\n",
    "    # Analisis de discriminate: 85.6%\n",
    "    # Maquina vector soporte:86.6 %\n",
    "def metricas(Xt, Xv, yt, yv, modelo):\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    yhat_train = modelo.predict(Xt)\n",
    "    yhat_val = modelo.predict(Xv)\n",
    "    print('Entrenamiento')\n",
    "    print(confusion_matrix(yt, yhat_train))\n",
    "    print(classification_report(yt, yhat_train))\n",
    "    print('Validación')\n",
    "    print(confusion_matrix(yv, yhat_val))\n",
    "    print(classification_report(yv, yhat_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1354,
   "id": "3b9082c2-7b60-45fa-a6b5-bc0ac5858d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(915, 5)\n",
      "(915,)\n",
      "188 valores nulos en Xt\n",
      "0 valores nulos en yt\n"
     ]
    }
   ],
   "source": [
    "print(Xt.shape)\n",
    "print(yt.shape)\n",
    "print(Xt.isnull().sum().sum(), 'valores nulos en Xt')\n",
    "print(yt.isnull().sum(), 'valores nulos en yt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1355,
   "id": "7036b879-1fbb-48b9-b3e0-7e8783e4848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rellenar valores nulos en Xt con la media de cada columna\n",
    "Xt = Xt.fillna(Xt.mean())\n",
    "Xv = Xv.fillna(Xv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1356,
   "id": "5ea26b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en ./Najera_maquinavectorsoporte.pkl\n",
      "Entrenamiento\n",
      "[[489  72]\n",
      " [167 187]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.87      0.80       561\n",
      "           1       0.72      0.53      0.61       354\n",
      "\n",
      "    accuracy                           0.74       915\n",
      "   macro avg       0.73      0.70      0.71       915\n",
      "weighted avg       0.74      0.74      0.73       915\n",
      "\n",
      "Validación\n",
      "[[213  34]\n",
      " [ 79  67]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.86      0.79       247\n",
      "           1       0.66      0.46      0.54       146\n",
      "\n",
      "    accuracy                           0.71       393\n",
      "   macro avg       0.70      0.66      0.67       393\n",
      "weighted avg       0.70      0.71      0.70       393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "modelo_mlpc.fit(Xt, yt)\n",
    "\n",
    "# Guardar el modelo\n",
    "import pickle\n",
    "\n",
    "filename = './Najera_maquinavectorsoporte.pkl'\n",
    "\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(modelo_mlpc, file)\n",
    "\n",
    "print(f'Modelo guardado en {filename}')\n",
    "\n",
    "# Mostrar métricas de entrenamiento y validación\n",
    "metricas(Xt, Xv, yt, yv, modelo_mlpc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1357,
   "id": "3d0eaa21-8146-4dc0-914d-2ee9742b13df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en ./Najera_maquinavectorsoporte.pkl\n",
      "Entrenamiento\n",
      "[[405 156]\n",
      " [152 202]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.72      0.72       561\n",
      "           1       0.56      0.57      0.57       354\n",
      "\n",
      "    accuracy                           0.66       915\n",
      "   macro avg       0.65      0.65      0.65       915\n",
      "weighted avg       0.66      0.66      0.66       915\n",
      "\n",
      "Validación\n",
      "[[178  69]\n",
      " [ 56  90]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.72      0.74       247\n",
      "           1       0.57      0.62      0.59       146\n",
      "\n",
      "    accuracy                           0.68       393\n",
      "   macro avg       0.66      0.67      0.67       393\n",
      "weighted avg       0.69      0.68      0.68       393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pickle\n",
    "\n",
    "# Aplicar SMOTE para balancear las clases en el conjunto de entrenamiento\n",
    "smote = SMOTE(random_state=42)\n",
    "Xt_res, yt_res = smote.fit_resample(Xt, yt)\n",
    "\n",
    "# Entrenar el modelo MLP con más iteraciones y mejor configuración\n",
    "modelo_mlpc = MLPClassifier(\n",
    "    max_iter=1500,\n",
    "    random_state=42,\n",
    "    activation='tanh',\n",
    "    solver='adam'\n",
    ")\n",
    "\n",
    "modelo_mlpc.fit(Xt_res, yt_res)\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "filename = './Najera_maquinavectorsoporte.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(modelo_mlpc, file)\n",
    "\n",
    "print(f'Modelo guardado en {filename}')\n",
    "\n",
    "# Mostrar métricas usando el conjunto original de validación\n",
    "metricas(Xt, Xv, yt, yv, modelo_mlpc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1358,
   "id": "f48e8539-8e1a-47e4-b0ea-668b329ee3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Random Forest =====\n",
      "Matriz de confusión:\n",
      "[[188  59]\n",
      " [ 71  75]]\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.76      0.74       247\n",
      "           1       0.56      0.51      0.54       146\n",
      "\n",
      "    accuracy                           0.67       393\n",
      "   macro avg       0.64      0.64      0.64       393\n",
      "weighted avg       0.66      0.67      0.67       393\n",
      "\n",
      "\n",
      "===== SVM =====\n",
      "Matriz de confusión:\n",
      "[[202  45]\n",
      " [ 64  82]]\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.82      0.79       247\n",
      "           1       0.65      0.56      0.60       146\n",
      "\n",
      "    accuracy                           0.72       393\n",
      "   macro avg       0.70      0.69      0.69       393\n",
      "weighted avg       0.72      0.72      0.72       393\n",
      "\n",
      "\n",
      "===== AdaBoost =====\n",
      "Matriz de confusión:\n",
      "[[178  69]\n",
      " [ 59  87]]\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.72      0.74       247\n",
      "           1       0.56      0.60      0.58       146\n",
      "\n",
      "    accuracy                           0.67       393\n",
      "   macro avg       0.65      0.66      0.66       393\n",
      "weighted avg       0.68      0.67      0.68       393\n",
      "\n",
      "\n",
      "===== LDA =====\n",
      "Matriz de confusión:\n",
      "[[180  67]\n",
      " [ 52  94]]\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.73      0.75       247\n",
      "           1       0.58      0.64      0.61       146\n",
      "\n",
      "    accuracy                           0.70       393\n",
      "   macro avg       0.68      0.69      0.68       393\n",
      "weighted avg       0.70      0.70      0.70       393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Diccionario de modelos\n",
    "modelos = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(kernel='rbf', probability=True, random_state=42),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "    'LDA': LinearDiscriminantAnalysis()\n",
    "}\n",
    "\n",
    "# Entrenar y evaluar cada modelo\n",
    "for nombre, modelo in modelos.items():\n",
    "    print(f'\\n===== {nombre} =====')\n",
    "    modelo.fit(Xt_res, yt_res)\n",
    "    y_pred = modelo.predict(Xv)\n",
    "    print('Matriz de confusión:')\n",
    "    print(confusion_matrix(yv, y_pred))\n",
    "    print('Reporte de clasificación:')\n",
    "    print(classification_report(yv, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d857b9-3c15-449b-abdf-a35c2f19f7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
