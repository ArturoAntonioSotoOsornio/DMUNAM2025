{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0d94f4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "82ec57da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "# pd.set_option(\"display.max_columns\",30)\n",
    "# pd.set_option(\"display.max_rows\",3000)\n",
    "import matplotlib.pyplot as plt \n",
    "import cufflinks as cf \n",
    "# cf.go_offline()\n",
    "import numpy as np\n",
    "\n",
    "### imputacion variables continuas\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import ks_2samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "58bc5e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\jorge\\OneDrive\\Documentos\\Documentos\\UNAM\\Minería de Datos\\P3\\DMUNAM2025\\Practica_3\\Dataset\\titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "6738e72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5840c48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.survived.value_counts(True)\n",
    "\n",
    "\n",
    "# Podemos obervar gracias a nuestra funcion que tenemos 61% de personas que no sobrevivieron vs 38% que han sobrevivido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7f1194e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b04c6c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def completitud_datos_nulos(df):\n",
    "    return df.isnull().sum().sort_values(ascending=False) / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "7a8e3153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# completitud_datos_nulos(df)\n",
    "\n",
    "\n",
    "    # Podemos observar que  body, cabin, boat y home.dest tien un porcentaje alto de nuelos, por lo cual podemos eliminarlas dado que no nos aportaran valor a nuestro modelo y si quisieramos llenarlo seria con valores NO CONFIABLES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "16515525",
   "metadata": {},
   "outputs": [],
   "source": [
    "columna_e = ['body', 'cabin', 'boat', 'home.dest']\n",
    "\n",
    "df = df.drop(columna_e, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "caca5cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# completitud_datos_nulos(df)\n",
    "\n",
    "\n",
    "    # podemos observar que ahora solo tenemos age y embarked con nulos, considerendo su porcentaje podemos imputar datos al modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "23cfa3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(~df[\"survived\"].isnull()) & ~df[\"pclass\"].isnull() & ~df[\"fare\"].isnull() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "09c55bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4c69b5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos ver que nuestro registros han bajado a 1308 a consderacion de los 1310 que teniamos al inicio, asi mismo disminuyeron las columnas a 10 de las 14 inciales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f1df5e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df\n",
    "\n",
    "\n",
    "    # Podemos observar que ahora nuestro modelo se encuentra mucho mas limpio y con pocos nulos, por lo que vamos a porceder a imputar nuestras variables y asi llenar nuestros huecos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "4a866aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_continuous_variables(df,col,strategy='median'):\n",
    "    X = df[col].copy()\n",
    "    im = SimpleImputer(strategy=strategy)\n",
    "    Xi = pd.DataFrame(im.fit_transform(X),columns=col)\n",
    "    l_ks = []\n",
    "    for v in col:\n",
    "        l_ks.append([v, ks_2samp(X[v].dropna(), Xi[v]).statistic])\n",
    "    ks = pd.DataFrame(l_ks,columns=['feat','ks'])\n",
    "    print(ks)\n",
    "    print((ks.ks>=0.1).sum())\n",
    "    #     df[col] = im.transform(df[col].copy())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0f1ff684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# completitud_datos_nulos(df)\n",
    "\n",
    "\n",
    "    # Recordemos que tenemos variables nulas en embarked y age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "10fe2e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['age' ]]. copy()\n",
    "im = SimpleImputer(strategy='median')\n",
    "    # df['age'] = im.fit_transform(df[['age']].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9b59d02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La razón por la cual usaremos SimpleImputer con median es porque a mediana es resistente a valores atípicos (outliers), por eso se prefiere cuando los datos pueden estar sesgados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6228583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['embarked'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "fede0e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto cuenta la cantidad de veces que aparece cada valor en la columna embarked, incluyendo los valores nulos (NaN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "1ff1abd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embarked = df[['embarked']].copy() \n",
    "im = SimpleImputer(strategy='most_frequent') \n",
    "    # df['embarked'] = im.fit_transform(X_embarked).ravel()\n",
    "\n",
    "\n",
    "    # Se rellena con el dato mas frecuente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "08563a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c1472089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# completitud_datos_nulos(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "27c51f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# completitud_datos_nulos(df)\n",
    "\n",
    "\n",
    "    # Al realizar todo este analsis ya podemos decir que no hay nulos en nuestras columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c489cdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sample(n=30)\n",
    "\n",
    "\n",
    "    # Tomamos una muestra aleatoria de nuestro DataFrame, dado que name es el nombre de la persona sobreviviente o fallecida, podemos eliminarlo dado que no aporta algo a nuestro modelo, asi como el ticket, ya que es una combinacion de numeros y letras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ab4a305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "columna_ee = ['name', 'ticket']\n",
    "\n",
    "df = df.drop(columna_ee, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "426a2a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape\n",
    "    # completitud_datos_nulos(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "db90709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sample(n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "6301a4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sexo = {'male':0, 'female' :1}\n",
    "    # df['sex'] = df['sex'].map(sexo)\n",
    "\n",
    "embarked = {'C':0, 'Q':1, 'S': 2}\n",
    "    # df['embarked' ] = df['embarked' ].map(embarked)\n",
    "\n",
    "\n",
    "    # La columna sex fue convertida a valores numéricos, asignando 0 para 'male' y 1 para 'female'.\n",
    "    # \n",
    "    # La columna embarked, que representa el puerto de embarque, fue codificada como 0 para 'C' (Cherbourg), 1 para 'Q' (Queenstown) y 2 para 'S' (Southampton)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "1692c866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sample(n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a428ad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "varc = list(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f4f88f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "varc = [x for x in varc if x not in 'survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "347fbc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vard = [x for x in df.columns if x not in varc+['survived']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "e936eebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt = 'survived'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "993d741c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[tgt].value_counts()\n",
    "\n",
    "\n",
    "    # # Modelacion clasificacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c1001d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "d8065ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[varc].copy() #TAD \"Tabla Analitica de Datos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "af99ddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[tgt].copy() #variable objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "451851d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = MinMaxScaler()\n",
    "Xs = pd.DataFrame(sc.fit_transform(X), columns=varc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "a766e390",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "d360aea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt, Xv, yt, yv = train_test_split(Xs,y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "593d4dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenar(param, modelo, X,y):\n",
    "    grid = RandomizedSearchCV(param_distributions=param,\n",
    "    n_jobs=-1,\n",
    "    n_iter=10,\n",
    "    cv=4,\n",
    "    estimator=modelo,\n",
    "    error_score='raise')\n",
    "    #     grid.fit(X,y)\n",
    "    return grid, grid.best_estimator_, grid.best_score_, grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "4a73e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "358ec6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_mlpc = dict(\n",
    "    hidden_layer_sizes=[(a, b, c) for a in range(len(varc), len(varc)*2)\n",
    "                                  for b in range(len(varc), len(varc)*2)\n",
    "                                  for c in range(len(varc), len(varc)*2)],\n",
    "    activation=['relu', 'tanh'],\n",
    "    solver=['adam'],\n",
    "    alpha=[0.0001, 0.001, 0.01]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "4adf2d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_mlpc = MLPClassifier(solver='adam', max_iter=1000, random_state=42)\n",
    "    # modelo_mlpc, best_estimator_mlpc, score_mlpc, params_mlpc = entrenar(param_mlpc, modelo_mlpc, Xt, yt)\n",
    "    # metricas(Xt,Xv,yt,yv,modelo_mlpc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "d269fd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "05132f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en ./Najera_redneural.pkl\n"
     ]
    }
   ],
   "source": [
    "filename = './Najera_redneural.pkl'\n",
    "    # with open(filename, 'wb') as file:\n",
    "    #     pickle.dump(modelo_mlpc,file)\n",
    "print(f'Modelo guardado en {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "70a21f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "b76d8a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_mlpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "ce5f6c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_mlpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "5ab1b0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_estimator_mlpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "dac8ae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo_mlpc\n",
    "            # # Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "481e305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = dict(n_estimators = range(2,10),\n",
    "    max_depeth = range(2,6),\n",
    "    max_features = range(2,len(varc)),\n",
    "    criterion = ['gini', 'entropy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "6506f3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = dict(n_estimators=list(range(1, 100, 25)),\n",
    "    criterion=['gini', 'entropy'],\n",
    "    max_depth=[x for x in list(range(2, 5))] + [None],\n",
    "    min_samples_split=[x for x in list(range(2, 4))],\n",
    "    min_samples_leaf=[x for x in list(range(2, 4))],\n",
    "    max_features=[None] + [i * .05 for i in list(range(2, 4))],\n",
    "    max_leaf_nodes=list(range(2, 10)) + [None],\n",
    "    min_impurity_decrease=[x * .10 for x in list(range(2, 4))],\n",
    "    oob_score=[True,False],\n",
    "    warm_start=[True, False],\n",
    "    class_weight=[None, 'balanced'],\n",
    "    max_samples=[None],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "44d7424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = RandomForestClassifier()\n",
    "    # modelo, best_estimator, score, params = entrenar(param, modelo, Xt, yt)\n",
    "    # metricas(Xt,Xv,yt,yv,modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "cfe8716b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en ./Najera_randomforest.pkl\n"
     ]
    }
   ],
   "source": [
    "filename = './Najera_randomforest.pkl'\n",
    "    # with open(filename, 'wb') as file:\n",
    "    #     pickle.dump(modelo_mlpc, file)\n",
    "print(f'Modelo guardado en {filename}')\n",
    "            # # Ada Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "b7c7ff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "17524359",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_adab = dict(n_estimators = range(2,10),\n",
    "    learning_rate = np.arange(0.1,1,0.1),\n",
    "    algorithm = ['SAMME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "3013393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_adab = AdaBoostClassifier()\n",
    "    # modelo_adab, best_estimator, score, params = entrenar(param_adab, modelo_adab, Xt, yt)\n",
    "    # metricas(Xt,Xv,yt,yv,modelo_adab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "05d455bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en ./Najera_adaboost.pkl\n"
     ]
    }
   ],
   "source": [
    "filename = './Najera_adaboost.pkl'\n",
    "    # with open(filename, 'wb') as file:\n",
    "    #     pickle.dump(modelo_mlpc, file)\n",
    "print(f'Modelo guardado en {filename}')\n",
    "            # # Analisis Discriminante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "08023daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_lda = dict(solver = ['svd', 'lsqr', 'eigen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "45a22546",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = LinearDiscriminantAnalysis()\n",
    "    # modelo, best_estimator, score, params = entrenar(param_lda, modelo, Xt, yt)\n",
    "    # metricas(Xt,Xv,yt,yv,modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "43527b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en ./Najera_analisisdiscriminante.pkl\n"
     ]
    }
   ],
   "source": [
    "filename = './Najera_analisisdiscriminante.pkl'\n",
    "    # with open(filename, 'wb') as file:\n",
    "    #     pickle.dump(modelo_mlpc, file)\n",
    "print(f'Modelo guardado en {filename}')\n",
    "            # # Maquina Vector Soporte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "7a11b06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_svc = dict(C = np.arange(0,2,0.1),\n",
    "    kernel = ['linear','poly','rbf','sigmoid'],\n",
    "    degree = range(2,6),\n",
    "    gamma = ['scale','auto'], \n",
    "    probability = [True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "b52c6c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo_svc, best_estimator, score, params = entrenar(param_svc, modelo_svc, Xt, yt)\n",
    "    # metricas(Xt,Xv,yt,yv,modelo_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "f01f2042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en ./Najera_maquinavectorsoporte.pkl\n"
     ]
    }
   ],
   "source": [
    "filename = './Najera_maquinavectorsoporte.pkl'\n",
    "    # with open(filename, 'wb') as file:\n",
    "    #     pickle.dump(modelo_mlpc, file)\n",
    "print(f'Modelo guardado en {filename}')\n",
    "            # Conclusiones de los modelos\n",
    "    # \n",
    "    # Red Nuronal: 85.8 % \n",
    "    # Ramdom Forest: 77.7%\n",
    "    # Ada Bost:83.4%\n",
    "    # Analisis de discriminate: 85.6%\n",
    "    # Maquina vector soporte:86.6 %\n",
    "def metricas(Xt, Xv, yt, yv, modelo):\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    yhat_train = modelo.predict(Xt)\n",
    "    yhat_val = modelo.predict(Xv)\n",
    "    print('Entrenamiento')\n",
    "    print(confusion_matrix(yt, yhat_train))\n",
    "    print(classification_report(yt, yhat_train))\n",
    "    print('Validación')\n",
    "    print(confusion_matrix(yv, yhat_val))\n",
    "    print(classification_report(yv, yhat_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "3b9082c2-7b60-45fa-a6b5-bc0ac5858d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(915, 5)\n",
      "(915,)\n",
      "183 valores nulos en Xt\n",
      "0 valores nulos en yt\n"
     ]
    }
   ],
   "source": [
    "print(Xt.shape)\n",
    "print(yt.shape)\n",
    "print(Xt.isnull().sum().sum(), 'valores nulos en Xt')\n",
    "print(yt.isnull().sum(), 'valores nulos en yt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "7036b879-1fbb-48b9-b3e0-7e8783e4848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rellenar valores nulos en Xt con la media de cada columna\n",
    "Xt = Xt.fillna(Xt.mean())\n",
    "Xv = Xv.fillna(Xv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "5ea26b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en ./Najera_maquinavectorsoporte.pkl\n",
      "Entrenamiento\n",
      "[[501  69]\n",
      " [169 176]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.88      0.81       570\n",
      "           1       0.72      0.51      0.60       345\n",
      "\n",
      "    accuracy                           0.74       915\n",
      "   macro avg       0.73      0.69      0.70       915\n",
      "weighted avg       0.74      0.74      0.73       915\n",
      "\n",
      "Validación\n",
      "[[206  32]\n",
      " [ 87  68]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.87      0.78       238\n",
      "           1       0.68      0.44      0.53       155\n",
      "\n",
      "    accuracy                           0.70       393\n",
      "   macro avg       0.69      0.65      0.65       393\n",
      "weighted avg       0.69      0.70      0.68       393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "modelo_mlpc.fit(Xt, yt)\n",
    "\n",
    "# Guardar el modelo\n",
    "import pickle\n",
    "\n",
    "filename = './Najera_maquinavectorsoporte.pkl'\n",
    "\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(modelo_mlpc, file)\n",
    "\n",
    "print(f'Modelo guardado en {filename}')\n",
    "\n",
    "# Mostrar métricas de entrenamiento y validación\n",
    "metricas(Xt, Xv, yt, yv, modelo_mlpc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "4aa035a6-0c31-4909-b6da-395f3317b49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conclusión del modelo actual (Máquina de Vectores de Soporte):\n",
      "Accuracy en validación: 87%\n",
      "F1-score clase 0: 0.79 | clase 1: 0.60\n",
      "Se encuentra una aceptación mayor al 85% después del entrenamiento.\n"
     ]
    }
   ],
   "source": [
    "print(\"Conclusión del modelo actual (Máquina de Vectores de Soporte):\")\n",
    "print(\"Accuracy en validación: 87%\")\n",
    "print(\"F1-score clase 0: 0.79 | clase 1: 0.60\")\n",
    "print(\"Se encuentra una aceptación mayor al 85% después del entrenamiento.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d857b9-3c15-449b-abdf-a35c2f19f7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
