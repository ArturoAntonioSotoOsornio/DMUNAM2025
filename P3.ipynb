{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LIBERIAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#De cajon\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Imputar y verificar\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "#Estandarizar y pca\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Entrenar y metricas (randimizecsearchcv, gridsearchcv, roc_auc_score)\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#Redes neuronales\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#Machines vector soporte\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#Arboles de decision\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Discriminante\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "#AdaBoost\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CARGA Y LIMPIEZA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset: (1310, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>survived</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "      <th>boat</th>\n",
       "      <th>body</th>\n",
       "      <th>home.dest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Allen, Miss. Elisabeth Walton</td>\n",
       "      <td>female</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24160</td>\n",
       "      <td>211.3375</td>\n",
       "      <td>B5</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>St Louis, MO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Allison, Master. Hudson Trevor</td>\n",
       "      <td>male</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pclass  survived                            name     sex      age  sibsp  \\\n",
       "0     1.0       1.0   Allen, Miss. Elisabeth Walton  female  29.0000    0.0   \n",
       "1     1.0       1.0  Allison, Master. Hudson Trevor    male   0.9167    1.0   \n",
       "\n",
       "   parch  ticket      fare    cabin embarked boat  body  \\\n",
       "0    0.0   24160  211.3375       B5        S    2   NaN   \n",
       "1    2.0  113781  151.5500  C22 C26        S   11   NaN   \n",
       "\n",
       "                         home.dest  \n",
       "0                     St Louis, MO  \n",
       "1  Montreal, PQ / Chesterville, ON  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar el dataset\n",
    "\n",
    "df = pd.read_csv(\"titanic.csv\")\n",
    "print('Tamaño del dataset:', df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables continuas:  ['pclass', 'age', 'sibsp', 'parch', 'fare', 'body']\n",
      "Variables discretas:  ['ticket', 'boat', 'sex', 'cabin', 'embarked', 'name', 'home.dest']\n",
      "Variable objetivo: survived\n"
     ]
    }
   ],
   "source": [
    "# Dividir las variables en continuas y discretas\n",
    "\n",
    "varc = list(df.describe().columns)\n",
    "varc.remove('survived')\n",
    "\n",
    "vard = list(set(df.columns) - set(varc))\n",
    "vard.remove('survived')\n",
    "\n",
    "print(\"Variables continuas: \", varc)\n",
    "print(\"Variables discretas: \", vard)\n",
    "print(\"Variable objetivo: survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EL genero podria ser una varibale interesante a tomar en cuenta en el modelo, por lo que se cambiara a numerica\n",
    "\n",
    "df['sex'] = df['sex'].apply(lambda x:1 if x == 'male' else 0)\n",
    "varc.append('sex') if 'sex' not in varc else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embarked podria ser una variable tambien interesante a tomar en cuenta en el modelo, por lo que se cambiara a numerica\n",
    "\n",
    "df = df.dropna(subset=['survived'])\n",
    "df['embarked'] = df['embarked'].apply(lambda x: 0 if x == 'C' else (1 if x == 'Q' else 2))\n",
    "varc.append('embarked') if 'embarked' not in varc else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "body        0.907563\n",
       "age         0.200917\n",
       "fare        0.000764\n",
       "pclass      0.000000\n",
       "parch       0.000000\n",
       "sibsp       0.000000\n",
       "sex         0.000000\n",
       "embarked    0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificar si hay valores nulos\n",
    "\n",
    "df[varc].isnull().sum().sort_values(ascending=False) / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar la columna ya que es asquerosa la cantidad de nulos que tiene\n",
    "\n",
    "df.drop('body', axis=1, inplace=True) if 'body' in df.columns else None\n",
    "df = df.dropna(subset=['survived'])\n",
    "varc = list(set(varc) - set(['body']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         0.200917\n",
       "fare        0.000764\n",
       "parch       0.000000\n",
       "pclass      0.000000\n",
       "sibsp       0.000000\n",
       "embarked    0.000000\n",
       "sex         0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificar si hay valores nulos\n",
    "\n",
    "df[varc].isnull().sum().sort_values(ascending=False) / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.10929409]), array([0.09796131]), array([0.12254768]), array([0.10929409])]\n"
     ]
    }
   ],
   "source": [
    "# Ver que opcion es mejor para imputar los valores nulos de la edad\n",
    "\n",
    "x = df[['age']].copy()\n",
    "\n",
    "im1 = SimpleImputer(strategy = 'mean')\n",
    "im2 = SimpleImputer(strategy = 'median')\n",
    "im3 = SimpleImputer(strategy = 'most_frequent')\n",
    "im4 = KNNImputer(n_neighbors = 5)\n",
    "\n",
    "x1 = pd.DataFrame(im1.fit_transform(x), columns = ['age'])\n",
    "x2 = pd.DataFrame(im2.fit_transform(x), columns = ['age'])\n",
    "x3 = pd.DataFrame(im3.fit_transform(x), columns = ['age'])\n",
    "x4 = pd.DataFrame(im4.fit_transform(x), columns = ['age'])\n",
    "\n",
    "\n",
    "ks = []\n",
    "\n",
    "ks.append(ks_2samp(x.dropna(), x1).statistic)\n",
    "ks.append(ks_2samp(x.dropna(), x2).statistic)\n",
    "ks.append(ks_2samp(x.dropna(), x3).statistic)\n",
    "ks.append(ks_2samp(x.dropna(), x4).statistic)\n",
    "\n",
    "print(ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fare        0.000764\n",
       "sibsp       0.000000\n",
       "parch       0.000000\n",
       "pclass      0.000000\n",
       "sex         0.000000\n",
       "embarked    0.000000\n",
       "age         0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#La mejor opcion segun statistic seria con la moda pero tomar en cuneta que el pvalue en todos los casos marcaba que no se cumplia la Ho\n",
    "\n",
    "df['age'] = x3['age']\n",
    "df[varc].isnull().sum().sort_values(ascending=False) / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulos de fare:  1\n"
     ]
    }
   ],
   "source": [
    "# Se imputa con lo que sea, namas es un valor\n",
    "\n",
    "print(\"Nulos de fare: \", df['fare'].isnull().sum())\n",
    "x = df[['fare']].copy()\n",
    "x1 = pd.DataFrame(im1.fit_transform(x), columns = ['fare'])\n",
    "\n",
    "df['fare'] = x1['fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parch       0.0\n",
       "sibsp       0.0\n",
       "pclass      0.0\n",
       "sex         0.0\n",
       "embarked    0.0\n",
       "fare        0.0\n",
       "age         0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificar si hay valores nulos\n",
    "\n",
    "df[varc].isnull().sum().sort_values(ascending=False) / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parch       float64\n",
       "sibsp       float64\n",
       "pclass      float64\n",
       "sex           int64\n",
       "embarked      int64\n",
       "fare        float64\n",
       "age         float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[varc].dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pasar a enteros\n",
    "\n",
    "df['sibsp'] = df['sibsp'].astype(int)\n",
    "df['parch'] = df['parch'].astype(int)\n",
    "df['pclass'] = df['pclass'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODELO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[varc].copy()\n",
    "y = df['survived'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se escala\n",
    "\n",
    "escalar = MinMaxScaler()\n",
    "df_e = escalar.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las funciones para entrenar y testear los modelos obtenidos\n",
    "\n",
    "def entrenar(param, modelo, X,y):\n",
    "    grid = RandomizedSearchCV(param_distributions=param,\n",
    "    n_jobs=-1,\n",
    "    n_iter=100,\n",
    "    cv=6,\n",
    "    estimator=modelo,\n",
    "    error_score='raise')\n",
    "    \n",
    "    grid.fit(X,y)\n",
    "    \n",
    "    return grid, grid.best_estimator_, grid.best_score_, grid.best_params_\n",
    "\n",
    "def metricas(Xt, Xv, yt, yv, modelo):\n",
    "    d = {'train':round(roc_auc_score(y_true=yt, y_score=modelo.predict_proba(Xt)[:,1]),3),\n",
    "    'validate':round(roc_auc_score(y_true=yv, y_score=modelo.predict_proba(Xv)[:,1]),3)}\n",
    "\n",
    "    print(d)\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sanchez\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': np.float64(0.839), 'validate': np.float64(0.835)}\n",
      "Componentes:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sanchez\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': np.float64(0.866), 'validate': np.float64(0.85)}\n",
      "Componentes:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sanchez\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': np.float64(0.856), 'validate': np.float64(0.847)}\n",
      "Componentes:  4\n",
      "{'train': np.float64(0.852), 'validate': np.float64(0.847)}\n",
      "Componentes:  5\n",
      "{'train': np.float64(0.848), 'validate': np.float64(0.846)}\n",
      "Componentes:  6\n",
      "{'train': np.float64(0.846), 'validate': np.float64(0.897)}\n",
      "Componentes:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sanchez\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Red neuronal, se aplica PCA y se ve que valores n mejor resultados de las componenetes.\n",
    "\n",
    "param_mlpc = dict(hidden_layer_sizes = [(a,b,c,) for a in range(len(varc), len(varc)*2) for b in range(len(varc), len(varc)*2) for c in range(len(varc), len(varc)*2)],\n",
    "             activation = ['identity', 'logistic', 'tanh', 'relu'],\n",
    "             solver = ['lbfgs', 'sgd', 'adam'],\n",
    "             alpha = np.arange(0.0001, 0.001, 0.0001),\n",
    "             learning_rate = ['constant', 'invscaling', 'adaptive'])\n",
    "\n",
    "for i in range(2,8):\n",
    "    pca = PCA(n_components = i)\n",
    "    df_pca = pd.DataFrame(pca.fit_transform(df_e))\n",
    "\n",
    "    Xt, Xv, yt, yv = train_test_split(df_pca,y, train_size=0.7)\n",
    "\n",
    "    modelo_mlpc = MLPClassifier()\n",
    "    modelo_mlpc, best_estimator_mlpc, score_mlpc, params_mlpc = entrenar(param_mlpc, modelo_mlpc, Xt, yt)\n",
    "    metricas(Xt,Xv,yt,yv,modelo_mlpc)\n",
    "    print(\"Componentes: \", i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': np.float64(0.817), 'validate': np.float64(0.815)}\n",
      "Componentes:  2\n",
      "{'train': np.float64(0.818), 'validate': np.float64(0.795)}\n",
      "Componentes:  3\n",
      "{'train': np.float64(0.801), 'validate': np.float64(0.766)}\n",
      "Componentes:  4\n",
      "{'train': np.float64(0.835), 'validate': np.float64(0.818)}\n",
      "Componentes:  5\n",
      "{'train': np.float64(0.847), 'validate': np.float64(0.825)}\n",
      "Componentes:  6\n",
      "{'train': np.float64(0.824), 'validate': np.float64(0.84)}\n",
      "Componentes:  7\n"
     ]
    }
   ],
   "source": [
    "# Maquina vector soporte, se aplica PCA y se ve que valores n mejor resultados de las componenetes.\n",
    "\n",
    "param_svc = dict(\n",
    "    C = np.arange(0.1, 2, 0.1),\n",
    "    kernel = ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    degree = range(2, 6),\n",
    "    gamma = ['scale', 'auto'],\n",
    "    probability = [True]\n",
    ")\n",
    "\n",
    "for i in range(2,8):\n",
    "    pca = PCA(n_components = i)\n",
    "    df_pca = pd.DataFrame(pca.fit_transform(df_e))\n",
    "\n",
    "    Xt, Xv, yt, yv = train_test_split(df_pca,y, train_size=0.7)\n",
    "\n",
    "    modelo_svc = SVC()\n",
    "    modelo_svc, best_estimator, score, params = entrenar(param_svc, modelo_svc, Xt, yt)\n",
    "    metricas(Xt,Xv,yt,yv,modelo_svc)\n",
    "    print(\"Componentes: \", i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': np.float64(0.821), 'validate': np.float64(0.823)}\n",
      "Componentes:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sanchez\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': np.float64(0.816), 'validate': np.float64(0.782)}\n",
      "Componentes:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sanchez\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': np.float64(0.804), 'validate': np.float64(0.798)}\n",
      "Componentes:  4\n",
      "{'train': np.float64(0.788), 'validate': np.float64(0.756)}\n",
      "Componentes:  5\n",
      "{'train': np.float64(0.772), 'validate': np.float64(0.749)}\n",
      "Componentes:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sanchez\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:861: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': np.float64(0.828), 'validate': np.float64(0.806)}\n",
      "Componentes:  7\n"
     ]
    }
   ],
   "source": [
    "# Random forest, se aplica PCA y se ve que valores n mejor resultados de las componenetes.\n",
    "\n",
    "param = dict(n_estimators=list(range(1, 100, 25)),\n",
    "                                    criterion=['gini', 'entropy'],\n",
    "                                    max_depth=[x for x in list(range(2, 5))] + [None],\n",
    "                                    min_samples_split=[x for x in list(range(2, 4))],\n",
    "                                    min_samples_leaf=[x for x in list(range(2, 4))],\n",
    "                                    max_features=[None] + [i * .05 for i in list(range(2, 4))],\n",
    "                                    max_leaf_nodes=list(range(2, 10)) + [None],\n",
    "                                    min_impurity_decrease=[x * .10 for x in list(range(2, 4))],\n",
    "                                    oob_score=[True,False],\n",
    "                                    warm_start=[True, False],\n",
    "                                    class_weight=[None, 'balanced'],\n",
    "                                    max_samples=[None],)\n",
    "\n",
    "\n",
    "for i in range(2,8):\n",
    "    pca = PCA(n_components = i)\n",
    "    df_pca = pd.DataFrame(pca.fit_transform(df_e))\n",
    "\n",
    "    Xt, Xv, yt, yv = train_test_split(df_pca,y, train_size=0.7)\n",
    "\n",
    "    modelo = RandomForestClassifier()\n",
    "    modelo, best_estimator, score, params = entrenar(param, modelo, Xt, yt)\n",
    "    metricas(Xt,Xv,yt,yv,modelo)\n",
    "    print(\"Componentes: \", i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': np.float64(0.822), 'validate': np.float64(0.822)}\n",
      "Componentes:  2\n",
      "{'train': np.float64(0.819), 'validate': np.float64(0.821)}\n",
      "Componentes:  3\n",
      "{'train': np.float64(0.838), 'validate': np.float64(0.809)}\n",
      "Componentes:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sanchez\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 3 is smaller than n_iter=100. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sanchez\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 3 is smaller than n_iter=100. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sanchez\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 3 is smaller than n_iter=100. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sanchez\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 3 is smaller than n_iter=100. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': np.float64(0.838), 'validate': np.float64(0.845)}\n",
      "Componentes:  5\n",
      "{'train': np.float64(0.854), 'validate': np.float64(0.804)}\n",
      "Componentes:  6\n",
      "{'train': np.float64(0.843), 'validate': np.float64(0.835)}\n",
      "Componentes:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sanchez\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 3 is smaller than n_iter=100. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sanchez\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 3 is smaller than n_iter=100. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Analisis del dicriminante, se aplica PCA y se ve que valores n mejor resultados de las componenetes.\n",
    "\n",
    "param_lda = dict(solver = ['svd', 'lsqr', 'eigen'])\n",
    "\n",
    "for i in range(2,8):\n",
    "    pca = PCA(n_components = i)\n",
    "    df_pca = pd.DataFrame(pca.fit_transform(df_e))\n",
    "\n",
    "    Xt, Xv, yt, yv = train_test_split(df_pca,y, train_size=0.7)\n",
    "\n",
    "    modelo = LinearDiscriminantAnalysis()\n",
    "    modelo, best_estimator, score, params = entrenar(param_lda, modelo, Xt, yt)\n",
    "    metricas(Xt,Xv,yt,yv,modelo)\n",
    "    print(\"Componentes: \", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sanchez\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 72 is smaller than n_iter=100. Running 72 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sanchez\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sanchez\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 72 is smaller than n_iter=100. Running 72 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': np.float64(0.761), 'validate': np.float64(0.761)}\n",
      "Componentes:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sanchez\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sanchez\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 72 is smaller than n_iter=100. Running 72 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': np.float64(0.824), 'validate': np.float64(0.829)}\n",
      "Componentes:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sanchez\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sanchez\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 72 is smaller than n_iter=100. Running 72 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': np.float64(0.766), 'validate': np.float64(0.749)}\n",
      "Componentes:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sanchez\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sanchez\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 72 is smaller than n_iter=100. Running 72 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': np.float64(0.758), 'validate': np.float64(0.754)}\n",
      "Componentes:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sanchez\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sanchez\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 72 is smaller than n_iter=100. Running 72 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': np.float64(0.763), 'validate': np.float64(0.753)}\n",
      "Componentes:  6\n",
      "{'train': np.float64(0.791), 'validate': np.float64(0.83)}\n",
      "Componentes:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sanchez\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#AdaBoost, se aplica PCA y se ve que valores n mejor resultados de las componenetes.\n",
    "\n",
    "param_adab = dict(n_estimators = range(2,10),\n",
    "             learning_rate = np.arange(0.1,1,0.1),\n",
    "             algorithm = ['SAMME'])\n",
    "\n",
    "for i in range(2,8):\n",
    "    pca = PCA(n_components = i)\n",
    "    df_pca = pd.DataFrame(pca.fit_transform(df_e))\n",
    "\n",
    "    Xt, Xv, yt, yv = train_test_split(df_pca,y, train_size=0.7)\n",
    "\n",
    "    modelo_adab = AdaBoostClassifier()\n",
    "    modelo_adab, best_estimator, score, params = entrenar(param_adab, modelo_adab, Xt, yt)\n",
    "    metricas(Xt,Xv,yt,yv,modelo_adab)\n",
    "    print(\"Componentes: \", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mejor valor obtenido del modelo fue con la red neuronal con 7 componentes con la qu ese obtuvo:\n",
    "\n",
    "train: 84.6 %\n",
    "validate: 89.7 %"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
